============================================
Starting job: MediumQA_1214
Model size: medium
Date: Sun 14 Dec 11:44:08 GMT 2025
============================================
Sun Dec 14 11:44:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:D8:00.0 Off |                    0 |
| N/A   31C    P0             24W /  250W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Starting Python script for MediumQA model...
--- Loaded configuration 'medium' from '/home/zceccgr/Scratch/freezeLLM/llm_scale_up/config.json' ---
Using device: cuda
Loading standard tokenizer ('bert-base-uncased')...
Standard vocabulary size: 30522
PAD token ID: 0
Instantiated Base Model: MediumQA with 80,538,624 trainable parameters.
Best pre-trained model will be saved to: models/MediumQA_20251214-114417/toy_llm_unified_pretrained.pth

--- Starting CLM Pre-training on nq_open for 'MediumQA' model ---
Pre-training batch size: 4 | Gradient accumulation: 8 | Effective batch size: 32
--- End of Pre-train Epoch 1 | Train Loss: 4.9033 | Validation Loss: 4.5955 | LR: 0.000100 ---
Validation loss improved. Saving model to models/MediumQA_20251214-114417/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 2 | Train Loss: 4.2628 | Validation Loss: 4.3565 | LR: 0.000100 ---
Validation loss improved. Saving model to models/MediumQA_20251214-114417/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 3 | Train Loss: 3.9498 | Validation Loss: 4.2387 | LR: 0.000100 ---
Validation loss improved. Saving model to models/MediumQA_20251214-114417/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 4 | Train Loss: 3.6970 | Validation Loss: 4.1814 | LR: 0.000100 ---
Validation loss improved. Saving model to models/MediumQA_20251214-114417/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 5 | Train Loss: 3.4725 | Validation Loss: 4.1615 | LR: 0.000100 ---
Validation loss improved. Saving model to models/MediumQA_20251214-114417/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 6 | Train Loss: 3.2668 | Validation Loss: 4.1654 | LR: 0.000100 ---
--- End of Pre-train Epoch 7 | Train Loss: 3.0756 | Validation Loss: 4.1835 | LR: 0.000100 ---
--- End of Pre-train Epoch 8 | Train Loss: 2.8996 | Validation Loss: 4.2264 | LR: 0.000100 ---
--- End of Pre-train Epoch 9 | Train Loss: 2.6591 | Validation Loss: 4.2161 | LR: 0.000050 ---
--- End of Pre-train Epoch 10 | Train Loss: 2.5348 | Validation Loss: 4.2544 | LR: 0.000050 ---

Early stopping triggered after 5 epochs without improvement.

Loading best pretrained model from models/MediumQA_20251214-114417/toy_llm_unified_pretrained.pth

Testing prompt completion after pretraining:
Prompt: 'Question: where was the statue of liberty originally built'
Model completion: question : where was the statue of liberty originally built in new york answer 18 kai text

--- Starting Fine-tuning on QA-SRL for 'MediumQA' model ---
Successfully loaded pre-trained weights into the QA model.
Loading and processing qa_srl dataset for 'train' split...
Finished processing 'train'. Found 5031 valid QA examples.
Loading and processing qa_srl dataset for 'validation' split...
Finished processing 'validation'. Found 1686 valid QA examples.
Starting fine-tuning for up to 20 epochs...
Batch size: 4 | Gradient accumulation steps: 8 | Effective batch size: 32
Early stopping patience: 3 epochs
Finetune Epoch 1 | Batch 50/1258 | Avg Loss: 6.1557
Finetune Epoch 1 | Batch 100/1258 | Avg Loss: 5.9690
Finetune Epoch 1 | Batch 150/1258 | Avg Loss: 5.7759
Finetune Epoch 1 | Batch 200/1258 | Avg Loss: 5.5762
Finetune Epoch 1 | Batch 250/1258 | Avg Loss: 5.4179
Finetune Epoch 1 | Batch 300/1258 | Avg Loss: 5.2843
Finetune Epoch 1 | Batch 350/1258 | Avg Loss: 5.1685
Finetune Epoch 1 | Batch 400/1258 | Avg Loss: 5.0693
Finetune Epoch 1 | Batch 450/1258 | Avg Loss: 4.9817
Finetune Epoch 1 | Batch 500/1258 | Avg Loss: 4.8991
Finetune Epoch 1 | Batch 550/1258 | Avg Loss: 4.8359
Finetune Epoch 1 | Batch 600/1258 | Avg Loss: 4.7578
Finetune Epoch 1 | Batch 650/1258 | Avg Loss: 4.7095
Finetune Epoch 1 | Batch 700/1258 | Avg Loss: 4.6593
Finetune Epoch 1 | Batch 750/1258 | Avg Loss: 4.6029
Finetune Epoch 1 | Batch 800/1258 | Avg Loss: 4.5569
Finetune Epoch 1 | Batch 850/1258 | Avg Loss: 4.5165
Finetune Epoch 1 | Batch 900/1258 | Avg Loss: 4.4805
Finetune Epoch 1 | Batch 950/1258 | Avg Loss: 4.4419
Finetune Epoch 1 | Batch 1000/1258 | Avg Loss: 4.4084
Finetune Epoch 1 | Batch 1050/1258 | Avg Loss: 4.3769
Finetune Epoch 1 | Batch 1100/1258 | Avg Loss: 4.3445
Finetune Epoch 1 | Batch 1150/1258 | Avg Loss: 4.3190
Finetune Epoch 1 | Batch 1200/1258 | Avg Loss: 4.2911
Finetune Epoch 1 | Batch 1250/1258 | Avg Loss: 4.2627
--- End of Finetune Epoch 1 | Train Loss: 4.2602 | Val Loss: 3.6152 | Val Acc: 0.0042 | Val F1: 0.1203 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/MediumQA_20251214-114417/toy_llm_qasrl_finetuned.pth
Finetune Epoch 2 | Batch 50/1258 | Avg Loss: 3.5529
Finetune Epoch 2 | Batch 100/1258 | Avg Loss: 3.4776
Finetune Epoch 2 | Batch 150/1258 | Avg Loss: 3.4978
Finetune Epoch 2 | Batch 200/1258 | Avg Loss: 3.4911
Finetune Epoch 2 | Batch 250/1258 | Avg Loss: 3.4906
Finetune Epoch 2 | Batch 300/1258 | Avg Loss: 3.4875
Finetune Epoch 2 | Batch 350/1258 | Avg Loss: 3.4838
Finetune Epoch 2 | Batch 400/1258 | Avg Loss: 3.4938
Finetune Epoch 2 | Batch 450/1258 | Avg Loss: 3.4940
Finetune Epoch 2 | Batch 500/1258 | Avg Loss: 3.4822
Finetune Epoch 2 | Batch 550/1258 | Avg Loss: 3.4798
Finetune Epoch 2 | Batch 600/1258 | Avg Loss: 3.4858
Finetune Epoch 2 | Batch 650/1258 | Avg Loss: 3.4802
Finetune Epoch 2 | Batch 700/1258 | Avg Loss: 3.4809
Finetune Epoch 2 | Batch 750/1258 | Avg Loss: 3.4833
Finetune Epoch 2 | Batch 800/1258 | Avg Loss: 3.4761
Finetune Epoch 2 | Batch 850/1258 | Avg Loss: 3.4741
Finetune Epoch 2 | Batch 900/1258 | Avg Loss: 3.4746
Finetune Epoch 2 | Batch 950/1258 | Avg Loss: 3.4747
Finetune Epoch 2 | Batch 1000/1258 | Avg Loss: 3.4733
Finetune Epoch 2 | Batch 1050/1258 | Avg Loss: 3.4666
Finetune Epoch 2 | Batch 1100/1258 | Avg Loss: 3.4661
Finetune Epoch 2 | Batch 1150/1258 | Avg Loss: 3.4612
Finetune Epoch 2 | Batch 1200/1258 | Avg Loss: 3.4607
Finetune Epoch 2 | Batch 1250/1258 | Avg Loss: 3.4583
--- End of Finetune Epoch 2 | Train Loss: 3.4588 | Val Loss: 3.4571 | Val Acc: 0.0065 | Val F1: 0.1317 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/MediumQA_20251214-114417/toy_llm_qasrl_finetuned.pth
Finetune Epoch 3 | Batch 50/1258 | Avg Loss: 3.3442
Finetune Epoch 3 | Batch 100/1258 | Avg Loss: 3.3003
Finetune Epoch 3 | Batch 150/1258 | Avg Loss: 3.2776
Finetune Epoch 3 | Batch 200/1258 | Avg Loss: 3.2658
Finetune Epoch 3 | Batch 250/1258 | Avg Loss: 3.2514
Finetune Epoch 3 | Batch 300/1258 | Avg Loss: 3.2594
Finetune Epoch 3 | Batch 350/1258 | Avg Loss: 3.2763
Finetune Epoch 3 | Batch 400/1258 | Avg Loss: 3.2677
Finetune Epoch 3 | Batch 450/1258 | Avg Loss: 3.2621
Finetune Epoch 3 | Batch 500/1258 | Avg Loss: 3.2636
Finetune Epoch 3 | Batch 550/1258 | Avg Loss: 3.2613
Finetune Epoch 3 | Batch 600/1258 | Avg Loss: 3.2619
Finetune Epoch 3 | Batch 650/1258 | Avg Loss: 3.2593
Finetune Epoch 3 | Batch 700/1258 | Avg Loss: 3.2561
Finetune Epoch 3 | Batch 750/1258 | Avg Loss: 3.2543
Finetune Epoch 3 | Batch 800/1258 | Avg Loss: 3.2642
Finetune Epoch 3 | Batch 850/1258 | Avg Loss: 3.2677
Finetune Epoch 3 | Batch 900/1258 | Avg Loss: 3.2646
Finetune Epoch 3 | Batch 950/1258 | Avg Loss: 3.2610
Finetune Epoch 3 | Batch 1000/1258 | Avg Loss: 3.2604
Finetune Epoch 3 | Batch 1050/1258 | Avg Loss: 3.2633
Finetune Epoch 3 | Batch 1100/1258 | Avg Loss: 3.2608
Finetune Epoch 3 | Batch 1150/1258 | Avg Loss: 3.2625
Finetune Epoch 3 | Batch 1200/1258 | Avg Loss: 3.2585
Finetune Epoch 3 | Batch 1250/1258 | Avg Loss: 3.2560
--- End of Finetune Epoch 3 | Train Loss: 3.2561 | Val Loss: 3.4978 | Val Acc: 0.0107 | Val F1: 0.1636 | LR: 0.000010 ---
No improvement for 1 epoch(s).
Finetune Epoch 4 | Batch 50/1258 | Avg Loss: 3.1322
Finetune Epoch 4 | Batch 100/1258 | Avg Loss: 3.0911
Finetune Epoch 4 | Batch 150/1258 | Avg Loss: 3.1066
Finetune Epoch 4 | Batch 200/1258 | Avg Loss: 3.1044
Finetune Epoch 4 | Batch 250/1258 | Avg Loss: 3.0960
Finetune Epoch 4 | Batch 300/1258 | Avg Loss: 3.1055
Finetune Epoch 4 | Batch 350/1258 | Avg Loss: 3.1018
Finetune Epoch 4 | Batch 400/1258 | Avg Loss: 3.1029
Finetune Epoch 4 | Batch 450/1258 | Avg Loss: 3.1087
Finetune Epoch 4 | Batch 500/1258 | Avg Loss: 3.0883
Finetune Epoch 4 | Batch 550/1258 | Avg Loss: 3.0848
Finetune Epoch 4 | Batch 600/1258 | Avg Loss: 3.0890
Finetune Epoch 4 | Batch 650/1258 | Avg Loss: 3.0942
Finetune Epoch 4 | Batch 700/1258 | Avg Loss: 3.1057
Finetune Epoch 4 | Batch 750/1258 | Avg Loss: 3.1117
Finetune Epoch 4 | Batch 800/1258 | Avg Loss: 3.1162
Finetune Epoch 4 | Batch 850/1258 | Avg Loss: 3.1149
Finetune Epoch 4 | Batch 900/1258 | Avg Loss: 3.1120
Finetune Epoch 4 | Batch 950/1258 | Avg Loss: 3.1138
Finetune Epoch 4 | Batch 1000/1258 | Avg Loss: 3.1136
Finetune Epoch 4 | Batch 1050/1258 | Avg Loss: 3.1162
Finetune Epoch 4 | Batch 1100/1258 | Avg Loss: 3.1140
Finetune Epoch 4 | Batch 1150/1258 | Avg Loss: 3.1169
Finetune Epoch 4 | Batch 1200/1258 | Avg Loss: 3.1175
Finetune Epoch 4 | Batch 1250/1258 | Avg Loss: 3.1173
--- End of Finetune Epoch 4 | Train Loss: 3.1175 | Val Loss: 3.4403 | Val Acc: 0.0113 | Val F1: 0.1794 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/MediumQA_20251214-114417/toy_llm_qasrl_finetuned.pth
Finetune Epoch 5 | Batch 50/1258 | Avg Loss: 2.9823
Finetune Epoch 5 | Batch 100/1258 | Avg Loss: 2.9747
Finetune Epoch 5 | Batch 150/1258 | Avg Loss: 2.9626
Finetune Epoch 5 | Batch 200/1258 | Avg Loss: 2.9673
Finetune Epoch 5 | Batch 250/1258 | Avg Loss: 2.9682
Finetune Epoch 5 | Batch 300/1258 | Avg Loss: 2.9722
Finetune Epoch 5 | Batch 350/1258 | Avg Loss: 2.9847
Finetune Epoch 5 | Batch 400/1258 | Avg Loss: 2.9953
Finetune Epoch 5 | Batch 450/1258 | Avg Loss: 2.9848
Finetune Epoch 5 | Batch 500/1258 | Avg Loss: 2.9887
Finetune Epoch 5 | Batch 550/1258 | Avg Loss: 2.9903
Finetune Epoch 5 | Batch 600/1258 | Avg Loss: 2.9855
Finetune Epoch 5 | Batch 650/1258 | Avg Loss: 2.9860
Finetune Epoch 5 | Batch 700/1258 | Avg Loss: 2.9844
Finetune Epoch 5 | Batch 750/1258 | Avg Loss: 2.9903
Finetune Epoch 5 | Batch 800/1258 | Avg Loss: 2.9867
Finetune Epoch 5 | Batch 850/1258 | Avg Loss: 2.9847
Finetune Epoch 5 | Batch 900/1258 | Avg Loss: 2.9848
Finetune Epoch 5 | Batch 950/1258 | Avg Loss: 2.9852
Finetune Epoch 5 | Batch 1000/1258 | Avg Loss: 2.9857
Finetune Epoch 5 | Batch 1050/1258 | Avg Loss: 2.9851
Finetune Epoch 5 | Batch 1100/1258 | Avg Loss: 2.9869
Finetune Epoch 5 | Batch 1150/1258 | Avg Loss: 2.9866
Finetune Epoch 5 | Batch 1200/1258 | Avg Loss: 2.9885
Finetune Epoch 5 | Batch 1250/1258 | Avg Loss: 2.9889
--- End of Finetune Epoch 5 | Train Loss: 2.9898 | Val Loss: 3.5560 | Val Acc: 0.0148 | Val F1: 0.1872 | LR: 0.000010 ---
No improvement for 1 epoch(s).
Finetune Epoch 6 | Batch 50/1258 | Avg Loss: 2.9562
Finetune Epoch 6 | Batch 100/1258 | Avg Loss: 2.9499
Finetune Epoch 6 | Batch 150/1258 | Avg Loss: 2.9146
Finetune Epoch 6 | Batch 200/1258 | Avg Loss: 2.9096
Finetune Epoch 6 | Batch 250/1258 | Avg Loss: 2.9012
Finetune Epoch 6 | Batch 300/1258 | Avg Loss: 2.8973
Finetune Epoch 6 | Batch 350/1258 | Avg Loss: 2.8784
Finetune Epoch 6 | Batch 400/1258 | Avg Loss: 2.8807
Finetune Epoch 6 | Batch 450/1258 | Avg Loss: 2.8615
Finetune Epoch 6 | Batch 500/1258 | Avg Loss: 2.8682
Finetune Epoch 6 | Batch 550/1258 | Avg Loss: 2.8627
Finetune Epoch 6 | Batch 600/1258 | Avg Loss: 2.8646
Finetune Epoch 6 | Batch 650/1258 | Avg Loss: 2.8501
Finetune Epoch 6 | Batch 700/1258 | Avg Loss: 2.8567
Finetune Epoch 6 | Batch 750/1258 | Avg Loss: 2.8562
Finetune Epoch 6 | Batch 800/1258 | Avg Loss: 2.8560
Finetune Epoch 6 | Batch 850/1258 | Avg Loss: 2.8523
Finetune Epoch 6 | Batch 900/1258 | Avg Loss: 2.8568
Finetune Epoch 6 | Batch 950/1258 | Avg Loss: 2.8530
Finetune Epoch 6 | Batch 1000/1258 | Avg Loss: 2.8520
Finetune Epoch 6 | Batch 1050/1258 | Avg Loss: 2.8569
Finetune Epoch 6 | Batch 1100/1258 | Avg Loss: 2.8520
Finetune Epoch 6 | Batch 1150/1258 | Avg Loss: 2.8512
Finetune Epoch 6 | Batch 1200/1258 | Avg Loss: 2.8523
Finetune Epoch 6 | Batch 1250/1258 | Avg Loss: 2.8538
--- End of Finetune Epoch 6 | Train Loss: 2.8518 | Val Loss: 3.5417 | Val Acc: 0.0190 | Val F1: 0.1788 | LR: 0.000010 ---
No improvement for 2 epoch(s).
Finetune Epoch 7 | Batch 50/1258 | Avg Loss: 2.7346
Finetune Epoch 7 | Batch 100/1258 | Avg Loss: 2.7506
Finetune Epoch 7 | Batch 150/1258 | Avg Loss: 2.7296
Finetune Epoch 7 | Batch 200/1258 | Avg Loss: 2.7304
Finetune Epoch 7 | Batch 250/1258 | Avg Loss: 2.7188
Finetune Epoch 7 | Batch 300/1258 | Avg Loss: 2.7286
Finetune Epoch 7 | Batch 350/1258 | Avg Loss: 2.7171
Finetune Epoch 7 | Batch 400/1258 | Avg Loss: 2.7072
Finetune Epoch 7 | Batch 450/1258 | Avg Loss: 2.7016
Finetune Epoch 7 | Batch 500/1258 | Avg Loss: 2.7097
Finetune Epoch 7 | Batch 550/1258 | Avg Loss: 2.7135
Finetune Epoch 7 | Batch 600/1258 | Avg Loss: 2.7181
Finetune Epoch 7 | Batch 650/1258 | Avg Loss: 2.7138
Finetune Epoch 7 | Batch 700/1258 | Avg Loss: 2.7200
Finetune Epoch 7 | Batch 750/1258 | Avg Loss: 2.7182
Finetune Epoch 7 | Batch 800/1258 | Avg Loss: 2.7205
Finetune Epoch 7 | Batch 850/1258 | Avg Loss: 2.7191
Finetune Epoch 7 | Batch 900/1258 | Avg Loss: 2.7176
Finetune Epoch 7 | Batch 950/1258 | Avg Loss: 2.7256
Finetune Epoch 7 | Batch 1000/1258 | Avg Loss: 2.7243
Finetune Epoch 7 | Batch 1050/1258 | Avg Loss: 2.7196
Finetune Epoch 7 | Batch 1100/1258 | Avg Loss: 2.7226
Finetune Epoch 7 | Batch 1150/1258 | Avg Loss: 2.7211
Finetune Epoch 7 | Batch 1200/1258 | Avg Loss: 2.7205
Finetune Epoch 7 | Batch 1250/1258 | Avg Loss: 2.7218
--- End of Finetune Epoch 7 | Train Loss: 2.7216 | Val Loss: 3.6593 | Val Acc: 0.0237 | Val F1: 0.1825 | LR: 0.000005 ---
No improvement for 3 epoch(s).

Early stopping triggered after 3 epochs without improvement.
Best validation loss: 3.4403 | Best F1: 0.1794

Fine-tuning finished.
Best fine-tuned QA model state_dict saved to models/MediumQA_20251214-114417/toy_llm_qasrl_finetuned.pth
Loading best model from models/MediumQA_20251214-114417/toy_llm_qasrl_finetuned.pth for final stats.
Final Validation Accuracy: 0.0113 | Final F1: 0.1794
Final training statistics saved to 'models/MediumQA_20251214-114417/finetune_stats.json'
============================================
Job completed: Sun 14 Dec 19:36:04 GMT 2025
============================================
