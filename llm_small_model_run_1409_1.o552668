Mon Sep 15 03:24:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:58:00.0 Off |                    0 |
| N/A   35C    P0             37W /  250W |       1MiB /  32768MiB |      2%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Starting Python script for smallqa model...
--- Loaded configuration 'small' from '/home/zceccgr/Scratch/freezeLLM/llm_scale_up/config.json' ---
Using device: cuda
Loading standard tokenizer ('bert-base-uncased')...
Standard vocabulary size: 30522
PAD token ID: 0
Instantiated Base Model: SmallQA with 11,038,720 trainable parameters.
Best pre-trained model will be saved to: models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth

--- Starting CLM Pre-training on nq_open for 'SmallQA' model ---
--- End of Pre-train Epoch 1 | Train Loss: 4.9449 | Validation Loss: 4.6978 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 2 | Train Loss: 4.4324 | Validation Loss: 4.5048 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 3 | Train Loss: 4.2413 | Validation Loss: 4.4099 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 4 | Train Loss: 4.1265 | Validation Loss: 4.3625 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 5 | Train Loss: 4.0499 | Validation Loss: 4.3271 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 6 | Train Loss: 3.9975 | Validation Loss: 4.3055 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 7 | Train Loss: 3.9570 | Validation Loss: 4.2819 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 8 | Train Loss: 3.9253 | Validation Loss: 4.2786 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 9 | Train Loss: 3.8996 | Validation Loss: 4.2726 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 10 | Train Loss: 3.8785 | Validation Loss: 4.2596 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 11 | Train Loss: 3.8614 | Validation Loss: 4.2538 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 12 | Train Loss: 3.8449 | Validation Loss: 4.2470 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 13 | Train Loss: 3.8325 | Validation Loss: 4.2407 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 14 | Train Loss: 3.8206 | Validation Loss: 4.2339 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 15 | Train Loss: 3.8099 | Validation Loss: 4.2372 | LR: 0.000299 ---
--- End of Pre-train Epoch 16 | Train Loss: 3.8003 | Validation Loss: 4.2258 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 17 | Train Loss: 3.7924 | Validation Loss: 4.2236 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 18 | Train Loss: 3.7846 | Validation Loss: 4.2260 | LR: 0.000299 ---
--- End of Pre-train Epoch 19 | Train Loss: 3.7768 | Validation Loss: 4.2247 | LR: 0.000299 ---
--- End of Pre-train Epoch 20 | Train Loss: 3.7700 | Validation Loss: 4.2161 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 21 | Train Loss: 3.7647 | Validation Loss: 4.2201 | LR: 0.000299 ---
--- End of Pre-train Epoch 22 | Train Loss: 3.7589 | Validation Loss: 4.2176 | LR: 0.000299 ---
--- End of Pre-train Epoch 23 | Train Loss: 3.7536 | Validation Loss: 4.2214 | LR: 0.000299 ---
--- End of Pre-train Epoch 24 | Train Loss: 3.5773 | Validation Loss: 4.1564 | LR: 0.000150 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 25 | Train Loss: 3.5239 | Validation Loss: 4.1591 | LR: 0.000150 ---
--- End of Pre-train Epoch 26 | Train Loss: 3.5101 | Validation Loss: 4.1586 | LR: 0.000150 ---
--- End of Pre-train Epoch 27 | Train Loss: 3.5050 | Validation Loss: 4.1576 | LR: 0.000150 ---
--- End of Pre-train Epoch 28 | Train Loss: 3.3871 | Validation Loss: 4.1364 | LR: 0.000075 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 29 | Train Loss: 3.3540 | Validation Loss: 4.1359 | LR: 0.000075 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 30 | Train Loss: 3.3410 | Validation Loss: 4.1386 | LR: 0.000075 ---
--- End of Pre-train Epoch 31 | Train Loss: 3.3325 | Validation Loss: 4.1391 | LR: 0.000075 ---
--- End of Pre-train Epoch 32 | Train Loss: 3.3276 | Validation Loss: 4.1419 | LR: 0.000075 ---
--- End of Pre-train Epoch 33 | Train Loss: 3.2500 | Validation Loss: 4.1346 | LR: 0.000037 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 34 | Train Loss: 3.2301 | Validation Loss: 4.1333 | LR: 0.000037 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 35 | Train Loss: 3.2203 | Validation Loss: 4.1364 | LR: 0.000037 ---
--- End of Pre-train Epoch 36 | Train Loss: 3.2135 | Validation Loss: 4.1386 | LR: 0.000037 ---
--- End of Pre-train Epoch 37 | Train Loss: 3.2069 | Validation Loss: 4.1387 | LR: 0.000037 ---
--- End of Pre-train Epoch 38 | Train Loss: 3.1608 | Validation Loss: 4.1323 | LR: 0.000019 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 39 | Train Loss: 3.1496 | Validation Loss: 4.1340 | LR: 0.000019 ---
--- End of Pre-train Epoch 40 | Train Loss: 3.1440 | Validation Loss: 4.1337 | LR: 0.000019 ---
--- End of Pre-train Epoch 41 | Train Loss: 3.1380 | Validation Loss: 4.1344 | LR: 0.000019 ---
--- End of Pre-train Epoch 42 | Train Loss: 3.1126 | Validation Loss: 4.1290 | LR: 0.000009 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 43 | Train Loss: 3.1070 | Validation Loss: 4.1350 | LR: 0.000009 ---
--- End of Pre-train Epoch 44 | Train Loss: 3.1032 | Validation Loss: 4.1286 | LR: 0.000009 ---
Validation loss improved. Saving model to models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 45 | Train Loss: 3.1003 | Validation Loss: 4.1346 | LR: 0.000009 ---
--- End of Pre-train Epoch 46 | Train Loss: 3.0967 | Validation Loss: 4.1317 | LR: 0.000009 ---
--- End of Pre-train Epoch 47 | Train Loss: 3.0939 | Validation Loss: 4.1346 | LR: 0.000009 ---
--- End of Pre-train Epoch 48 | Train Loss: 3.0819 | Validation Loss: 4.1311 | LR: 0.000005 ---
--- End of Pre-train Epoch 49 | Train Loss: 3.0787 | Validation Loss: 4.1302 | LR: 0.000005 ---

Early stopping triggered after 5 epochs without improvement.

Loading best pretrained model from models/SmallQA_20250915-032452/toy_llm_unified_pretrained.pth

Testing prompt completion after pretraining:
Prompt: 'Question: where was the statue of liberty originally built'
Model completion: question : where was the statue of liberty originally built by mariah carey answer : tha setup by the

--- Starting Fine-tuning on QA-SRL for 'SmallQA' model ---
Successfully loaded pre-trained weights into the QA model.
Loading and processing qa_srl dataset for 'train' split...
Finished processing 'train'. Found 5031 valid QA examples.
Loading and processing qa_srl dataset for 'validation' split...
Finished processing 'validation'. Found 1686 valid QA examples.
Starting fine-tuning for 3 epochs...
Finetune Epoch 1 | Batch 50/1258 | Avg Loss: 4.5912
Finetune Epoch 1 | Batch 100/1258 | Avg Loss: 4.2504
Finetune Epoch 1 | Batch 150/1258 | Avg Loss: 4.1071
Finetune Epoch 1 | Batch 200/1258 | Avg Loss: 4.0145
Finetune Epoch 1 | Batch 250/1258 | Avg Loss: 3.9311
Finetune Epoch 1 | Batch 300/1258 | Avg Loss: 3.8777
Finetune Epoch 1 | Batch 350/1258 | Avg Loss: 3.8284
Finetune Epoch 1 | Batch 400/1258 | Avg Loss: 3.7781
Finetune Epoch 1 | Batch 450/1258 | Avg Loss: 3.7505
Finetune Epoch 1 | Batch 500/1258 | Avg Loss: 3.7209
Finetune Epoch 1 | Batch 550/1258 | Avg Loss: 3.6906
Finetune Epoch 1 | Batch 600/1258 | Avg Loss: 3.6748
Finetune Epoch 1 | Batch 650/1258 | Avg Loss: 3.6604
Finetune Epoch 1 | Batch 700/1258 | Avg Loss: 3.6414
Finetune Epoch 1 | Batch 750/1258 | Avg Loss: 3.6229
Finetune Epoch 1 | Batch 800/1258 | Avg Loss: 3.6094
Finetune Epoch 1 | Batch 850/1258 | Avg Loss: 3.5975
Finetune Epoch 1 | Batch 900/1258 | Avg Loss: 3.5857
Finetune Epoch 1 | Batch 950/1258 | Avg Loss: 3.5765
Finetune Epoch 1 | Batch 1000/1258 | Avg Loss: 3.5678
Finetune Epoch 1 | Batch 1050/1258 | Avg Loss: 3.5611
Finetune Epoch 1 | Batch 1100/1258 | Avg Loss: 3.5527
Finetune Epoch 1 | Batch 1150/1258 | Avg Loss: 3.5429
Finetune Epoch 1 | Batch 1200/1258 | Avg Loss: 3.5380
Finetune Epoch 1 | Batch 1250/1258 | Avg Loss: 3.5293
--- End of Finetune Epoch 1 | Train Loss: 3.5277 | Val Loss: 3.3734 | Val Acc: 0.0083 | Val F1: 0.1861 | LR: 0.000050 ---
Validation loss improved. Saving best model to models/SmallQA_20250915-032452/toy_llm_qasrl_finetuned.pth
Finetune Epoch 2 | Batch 50/1258 | Avg Loss: 3.0831
Finetune Epoch 2 | Batch 100/1258 | Avg Loss: 3.1757
Finetune Epoch 2 | Batch 150/1258 | Avg Loss: 3.1648
Finetune Epoch 2 | Batch 200/1258 | Avg Loss: 3.1474
Finetune Epoch 2 | Batch 250/1258 | Avg Loss: 3.1486
Finetune Epoch 2 | Batch 300/1258 | Avg Loss: 3.1440
Finetune Epoch 2 | Batch 350/1258 | Avg Loss: 3.1326
Finetune Epoch 2 | Batch 400/1258 | Avg Loss: 3.1318
Finetune Epoch 2 | Batch 450/1258 | Avg Loss: 3.1207
Finetune Epoch 2 | Batch 500/1258 | Avg Loss: 3.1159
Finetune Epoch 2 | Batch 550/1258 | Avg Loss: 3.1182
Finetune Epoch 2 | Batch 600/1258 | Avg Loss: 3.1213
Finetune Epoch 2 | Batch 650/1258 | Avg Loss: 3.1164
Finetune Epoch 2 | Batch 700/1258 | Avg Loss: 3.1109
Finetune Epoch 2 | Batch 750/1258 | Avg Loss: 3.1069
Finetune Epoch 2 | Batch 800/1258 | Avg Loss: 3.1093
Finetune Epoch 2 | Batch 850/1258 | Avg Loss: 3.1081
Finetune Epoch 2 | Batch 900/1258 | Avg Loss: 3.0994
Finetune Epoch 2 | Batch 950/1258 | Avg Loss: 3.0985
Finetune Epoch 2 | Batch 1000/1258 | Avg Loss: 3.0951
Finetune Epoch 2 | Batch 1050/1258 | Avg Loss: 3.0938
Finetune Epoch 2 | Batch 1100/1258 | Avg Loss: 3.0856
Finetune Epoch 2 | Batch 1150/1258 | Avg Loss: 3.0851
Finetune Epoch 2 | Batch 1200/1258 | Avg Loss: 3.0816
Finetune Epoch 2 | Batch 1250/1258 | Avg Loss: 3.0835
--- End of Finetune Epoch 2 | Train Loss: 3.0817 | Val Loss: 3.5025 | Val Acc: 0.0356 | Val F1: 0.2027 | LR: 0.000050 ---
Finetune Epoch 3 | Batch 50/1258 | Avg Loss: 2.5933
Finetune Epoch 3 | Batch 100/1258 | Avg Loss: 2.6446
Finetune Epoch 3 | Batch 150/1258 | Avg Loss: 2.7123
Finetune Epoch 3 | Batch 200/1258 | Avg Loss: 2.7057
Finetune Epoch 3 | Batch 250/1258 | Avg Loss: 2.6962
Finetune Epoch 3 | Batch 300/1258 | Avg Loss: 2.7141
Finetune Epoch 3 | Batch 350/1258 | Avg Loss: 2.7026
Finetune Epoch 3 | Batch 400/1258 | Avg Loss: 2.6973
Finetune Epoch 3 | Batch 450/1258 | Avg Loss: 2.7069
Finetune Epoch 3 | Batch 500/1258 | Avg Loss: 2.7131
Finetune Epoch 3 | Batch 550/1258 | Avg Loss: 2.7165
Finetune Epoch 3 | Batch 600/1258 | Avg Loss: 2.7149
Finetune Epoch 3 | Batch 650/1258 | Avg Loss: 2.7103
Finetune Epoch 3 | Batch 700/1258 | Avg Loss: 2.7134
Finetune Epoch 3 | Batch 750/1258 | Avg Loss: 2.7152
Finetune Epoch 3 | Batch 800/1258 | Avg Loss: 2.7164
Finetune Epoch 3 | Batch 850/1258 | Avg Loss: 2.7148
Finetune Epoch 3 | Batch 900/1258 | Avg Loss: 2.7152
Finetune Epoch 3 | Batch 950/1258 | Avg Loss: 2.7199
Finetune Epoch 3 | Batch 1000/1258 | Avg Loss: 2.7148
Finetune Epoch 3 | Batch 1050/1258 | Avg Loss: 2.7180
Finetune Epoch 3 | Batch 1100/1258 | Avg Loss: 2.7234
Finetune Epoch 3 | Batch 1150/1258 | Avg Loss: 2.7172
Finetune Epoch 3 | Batch 1200/1258 | Avg Loss: 2.7229
Finetune Epoch 3 | Batch 1250/1258 | Avg Loss: 2.7300
--- End of Finetune Epoch 3 | Train Loss: 2.7300 | Val Loss: 3.7966 | Val Acc: 0.0267 | Val F1: 0.1962 | LR: 0.000050 ---

Fine-tuning finished.
Best fine-tuned QA model state_dict saved to models/SmallQA_20250915-032452/toy_llm_qasrl_finetuned.pth
Loading best model from models/SmallQA_20250915-032452/toy_llm_qasrl_finetuned.pth for final stats.
Final Validation Accuracy: 0.0083 | Final F1: 0.1861
Final training statistics saved to 'models/SmallQA_20250915-032452/finetune_stats.json'
Script finished.
