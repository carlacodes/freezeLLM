============================================
Starting job: SmallQA_1214
Model size: small
Date: Sun 14 Dec 21:46:19 GMT 2025
============================================
Sun Dec 14 21:46:19 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          On  |   00000000:06:00.0 Off |                    0 |
| N/A   33C    P0             32W /  250W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Starting Python script for SmallQA model...
--- Loaded configuration 'small' from '/home/zceccgr/Scratch/freezeLLM/llm_scale_up/config.json' ---
Using device: cuda
Loading standard tokenizer ('bert-base-uncased')...
Standard vocabulary size: 30522
PAD token ID: 0
Instantiated Base Model: SmallQA with 11,104,256 trainable parameters.
Best pre-trained model will be saved to: models/SmallQA_20251214-214625/toy_llm_unified_pretrained.pth

--- Starting CLM Pre-training on nq_open for 'SmallQA' model ---
Pre-training batch size: 8 | Gradient accumulation: 4 | Effective batch size: 32
--- End of Pre-train Epoch 1 | Train Loss: 4.9527 | Validation Loss: 4.6274 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20251214-214625/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 2 | Train Loss: 4.3230 | Validation Loss: 4.4146 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20251214-214625/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 3 | Train Loss: 4.0624 | Validation Loss: 4.3240 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20251214-214625/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 4 | Train Loss: 3.8793 | Validation Loss: 4.2766 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20251214-214625/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 5 | Train Loss: 3.7382 | Validation Loss: 4.2519 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20251214-214625/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 6 | Train Loss: 3.6228 | Validation Loss: 4.2414 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20251214-214625/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 7 | Train Loss: 3.5265 | Validation Loss: 4.2433 | LR: 0.000299 ---
--- End of Pre-train Epoch 8 | Train Loss: 3.4458 | Validation Loss: 4.2488 | LR: 0.000299 ---
--- End of Pre-train Epoch 9 | Train Loss: 3.3762 | Validation Loss: 4.2593 | LR: 0.000299 ---
--- End of Pre-train Epoch 10 | Train Loss: 3.2050 | Validation Loss: 4.2367 | LR: 0.000150 ---
Validation loss improved. Saving model to models/SmallQA_20251214-214625/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 11 | Train Loss: 3.1354 | Validation Loss: 4.2472 | LR: 0.000150 ---
--- End of Pre-train Epoch 12 | Train Loss: 3.0893 | Validation Loss: 4.2590 | LR: 0.000150 ---
--- End of Pre-train Epoch 13 | Train Loss: 3.0525 | Validation Loss: 4.2720 | LR: 0.000150 ---
--- End of Pre-train Epoch 14 | Train Loss: 2.9611 | Validation Loss: 4.2577 | LR: 0.000075 ---
--- End of Pre-train Epoch 15 | Train Loss: 2.9252 | Validation Loss: 4.2665 | LR: 0.000075 ---

Early stopping triggered after 5 epochs without improvement.

Loading best pretrained model from models/SmallQA_20251214-214625/toy_llm_unified_pretrained.pth

Testing prompt completion after pretraining:
Prompt: 'Question: where was the statue of liberty originally built'
Model completion: question : where was the statue of liberty originally built by the state of england answer : belfast

--- Starting Fine-tuning on QA-SRL for 'SmallQA' model ---
Successfully loaded pre-trained weights into the QA model.
Loading and processing qa_srl dataset for 'train' split...
Finished processing 'train'. Found 5031 valid QA examples.
Loading and processing qa_srl dataset for 'validation' split...
Finished processing 'validation'. Found 1686 valid QA examples.
Starting fine-tuning for up to 20 epochs...
Batch size: 8 | Gradient accumulation steps: 4 | Effective batch size: 32
Early stopping patience: 3 epochs
Finetune Epoch 1 | Batch 50/629 | Avg Loss: 6.1303
Finetune Epoch 1 | Batch 100/629 | Avg Loss: 5.9553
Finetune Epoch 1 | Batch 150/629 | Avg Loss: 5.7849
Finetune Epoch 1 | Batch 200/629 | Avg Loss: 5.6268
Finetune Epoch 1 | Batch 250/629 | Avg Loss: 5.4755
Finetune Epoch 1 | Batch 300/629 | Avg Loss: 5.3272
Finetune Epoch 1 | Batch 350/629 | Avg Loss: 5.2187
Finetune Epoch 1 | Batch 400/629 | Avg Loss: 5.1087
Finetune Epoch 1 | Batch 450/629 | Avg Loss: 5.0196
Finetune Epoch 1 | Batch 500/629 | Avg Loss: 4.9313
Finetune Epoch 1 | Batch 550/629 | Avg Loss: 4.8541
Finetune Epoch 1 | Batch 600/629 | Avg Loss: 4.7836
--- End of Finetune Epoch 1 | Train Loss: 4.7488 | Val Loss: 3.9078 | Val Acc: 0.0047 | Val F1: 0.1176 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/SmallQA_20251214-214625/toy_llm_qasrl_finetuned.pth
Finetune Epoch 2 | Batch 50/629 | Avg Loss: 3.9728
Finetune Epoch 2 | Batch 100/629 | Avg Loss: 3.9320
Finetune Epoch 2 | Batch 150/629 | Avg Loss: 3.9187
Finetune Epoch 2 | Batch 200/629 | Avg Loss: 3.9036
Finetune Epoch 2 | Batch 250/629 | Avg Loss: 3.8840
Finetune Epoch 2 | Batch 300/629 | Avg Loss: 3.8760
Finetune Epoch 2 | Batch 350/629 | Avg Loss: 3.8675
Finetune Epoch 2 | Batch 400/629 | Avg Loss: 3.8502
Finetune Epoch 2 | Batch 450/629 | Avg Loss: 3.8288
Finetune Epoch 2 | Batch 500/629 | Avg Loss: 3.8201
Finetune Epoch 2 | Batch 550/629 | Avg Loss: 3.8076
Finetune Epoch 2 | Batch 600/629 | Avg Loss: 3.7919
--- End of Finetune Epoch 2 | Train Loss: 3.7851 | Val Loss: 3.6052 | Val Acc: 0.0047 | Val F1: 0.1155 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/SmallQA_20251214-214625/toy_llm_qasrl_finetuned.pth
Finetune Epoch 3 | Batch 50/629 | Avg Loss: 3.5517
Finetune Epoch 3 | Batch 100/629 | Avg Loss: 3.5622
Finetune Epoch 3 | Batch 150/629 | Avg Loss: 3.5765
Finetune Epoch 3 | Batch 200/629 | Avg Loss: 3.5845
Finetune Epoch 3 | Batch 250/629 | Avg Loss: 3.5808
Finetune Epoch 3 | Batch 300/629 | Avg Loss: 3.5881
Finetune Epoch 3 | Batch 350/629 | Avg Loss: 3.5845
Finetune Epoch 3 | Batch 400/629 | Avg Loss: 3.5782
Finetune Epoch 3 | Batch 450/629 | Avg Loss: 3.5745
Finetune Epoch 3 | Batch 500/629 | Avg Loss: 3.5688
Finetune Epoch 3 | Batch 550/629 | Avg Loss: 3.5660
Finetune Epoch 3 | Batch 600/629 | Avg Loss: 3.5574
--- End of Finetune Epoch 3 | Train Loss: 3.5545 | Val Loss: 3.5041 | Val Acc: 0.0053 | Val F1: 0.1441 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/SmallQA_20251214-214625/toy_llm_qasrl_finetuned.pth
Finetune Epoch 4 | Batch 50/629 | Avg Loss: 3.4021
Finetune Epoch 4 | Batch 100/629 | Avg Loss: 3.4220
Finetune Epoch 4 | Batch 150/629 | Avg Loss: 3.4453
Finetune Epoch 4 | Batch 200/629 | Avg Loss: 3.4587
Finetune Epoch 4 | Batch 250/629 | Avg Loss: 3.4561
Finetune Epoch 4 | Batch 300/629 | Avg Loss: 3.4460
Finetune Epoch 4 | Batch 350/629 | Avg Loss: 3.4471
Finetune Epoch 4 | Batch 400/629 | Avg Loss: 3.4483
Finetune Epoch 4 | Batch 450/629 | Avg Loss: 3.4523
Finetune Epoch 4 | Batch 500/629 | Avg Loss: 3.4543
Finetune Epoch 4 | Batch 550/629 | Avg Loss: 3.4581
Finetune Epoch 4 | Batch 600/629 | Avg Loss: 3.4564
--- End of Finetune Epoch 4 | Train Loss: 3.4557 | Val Loss: 3.4528 | Val Acc: 0.0089 | Val F1: 0.1557 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/SmallQA_20251214-214625/toy_llm_qasrl_finetuned.pth
Finetune Epoch 5 | Batch 50/629 | Avg Loss: 3.4194
Finetune Epoch 5 | Batch 100/629 | Avg Loss: 3.4124
Finetune Epoch 5 | Batch 150/629 | Avg Loss: 3.3903
Finetune Epoch 5 | Batch 200/629 | Avg Loss: 3.3810
Finetune Epoch 5 | Batch 250/629 | Avg Loss: 3.3784
Finetune Epoch 5 | Batch 300/629 | Avg Loss: 3.3825
Finetune Epoch 5 | Batch 350/629 | Avg Loss: 3.3857
Finetune Epoch 5 | Batch 400/629 | Avg Loss: 3.3863
Finetune Epoch 5 | Batch 450/629 | Avg Loss: 3.3850
Finetune Epoch 5 | Batch 500/629 | Avg Loss: 3.3833
Finetune Epoch 5 | Batch 550/629 | Avg Loss: 3.3795
Finetune Epoch 5 | Batch 600/629 | Avg Loss: 3.3751
--- End of Finetune Epoch 5 | Train Loss: 3.3740 | Val Loss: 3.4287 | Val Acc: 0.0083 | Val F1: 0.1601 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/SmallQA_20251214-214625/toy_llm_qasrl_finetuned.pth
Finetune Epoch 6 | Batch 50/629 | Avg Loss: 3.3337
Finetune Epoch 6 | Batch 100/629 | Avg Loss: 3.3482
Finetune Epoch 6 | Batch 150/629 | Avg Loss: 3.3206
Finetune Epoch 6 | Batch 200/629 | Avg Loss: 3.3154
Finetune Epoch 6 | Batch 250/629 | Avg Loss: 3.3292
Finetune Epoch 6 | Batch 300/629 | Avg Loss: 3.3300
Finetune Epoch 6 | Batch 350/629 | Avg Loss: 3.3288
Finetune Epoch 6 | Batch 400/629 | Avg Loss: 3.3336
Finetune Epoch 6 | Batch 450/629 | Avg Loss: 3.3361
Finetune Epoch 6 | Batch 500/629 | Avg Loss: 3.3302
Finetune Epoch 6 | Batch 550/629 | Avg Loss: 3.3296
Finetune Epoch 6 | Batch 600/629 | Avg Loss: 3.3271
--- End of Finetune Epoch 6 | Train Loss: 3.3283 | Val Loss: 3.4036 | Val Acc: 0.0083 | Val F1: 0.1628 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/SmallQA_20251214-214625/toy_llm_qasrl_finetuned.pth
Finetune Epoch 7 | Batch 50/629 | Avg Loss: 3.2617
Finetune Epoch 7 | Batch 100/629 | Avg Loss: 3.2569
Finetune Epoch 7 | Batch 150/629 | Avg Loss: 3.2650
Finetune Epoch 7 | Batch 200/629 | Avg Loss: 3.2647
Finetune Epoch 7 | Batch 250/629 | Avg Loss: 3.2600
Finetune Epoch 7 | Batch 300/629 | Avg Loss: 3.2613
Finetune Epoch 7 | Batch 350/629 | Avg Loss: 3.2619
Finetune Epoch 7 | Batch 400/629 | Avg Loss: 3.2714
Finetune Epoch 7 | Batch 450/629 | Avg Loss: 3.2705
Finetune Epoch 7 | Batch 500/629 | Avg Loss: 3.2650
Finetune Epoch 7 | Batch 550/629 | Avg Loss: 3.2641
Finetune Epoch 7 | Batch 600/629 | Avg Loss: 3.2615
--- End of Finetune Epoch 7 | Train Loss: 3.2640 | Val Loss: 3.4035 | Val Acc: 0.0101 | Val F1: 0.1764 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/SmallQA_20251214-214625/toy_llm_qasrl_finetuned.pth
Finetune Epoch 8 | Batch 50/629 | Avg Loss: 3.2238
Finetune Epoch 8 | Batch 100/629 | Avg Loss: 3.2354
Finetune Epoch 8 | Batch 150/629 | Avg Loss: 3.2260
Finetune Epoch 8 | Batch 200/629 | Avg Loss: 3.2286
Finetune Epoch 8 | Batch 250/629 | Avg Loss: 3.2302
Finetune Epoch 8 | Batch 300/629 | Avg Loss: 3.2229
Finetune Epoch 8 | Batch 350/629 | Avg Loss: 3.2318
Finetune Epoch 8 | Batch 400/629 | Avg Loss: 3.2336
Finetune Epoch 8 | Batch 450/629 | Avg Loss: 3.2302
Finetune Epoch 8 | Batch 500/629 | Avg Loss: 3.2256
Finetune Epoch 8 | Batch 550/629 | Avg Loss: 3.2282
Finetune Epoch 8 | Batch 600/629 | Avg Loss: 3.2242
--- End of Finetune Epoch 8 | Train Loss: 3.2216 | Val Loss: 3.4055 | Val Acc: 0.0077 | Val F1: 0.1733 | LR: 0.000010 ---
No improvement for 1 epoch(s).
Finetune Epoch 9 | Batch 50/629 | Avg Loss: 3.1402
Finetune Epoch 9 | Batch 100/629 | Avg Loss: 3.1542
Finetune Epoch 9 | Batch 150/629 | Avg Loss: 3.1598
Finetune Epoch 9 | Batch 200/629 | Avg Loss: 3.1647
Finetune Epoch 9 | Batch 250/629 | Avg Loss: 3.1688
Finetune Epoch 9 | Batch 300/629 | Avg Loss: 3.1736
Finetune Epoch 9 | Batch 350/629 | Avg Loss: 3.1843
Finetune Epoch 9 | Batch 400/629 | Avg Loss: 3.1876
Finetune Epoch 9 | Batch 450/629 | Avg Loss: 3.1894
Finetune Epoch 9 | Batch 500/629 | Avg Loss: 3.1877
Finetune Epoch 9 | Batch 550/629 | Avg Loss: 3.1826
Finetune Epoch 9 | Batch 600/629 | Avg Loss: 3.1836
--- End of Finetune Epoch 9 | Train Loss: 3.1829 | Val Loss: 3.4041 | Val Acc: 0.0089 | Val F1: 0.1782 | LR: 0.000005 ---
No improvement for 2 epoch(s).
Finetune Epoch 10 | Batch 50/629 | Avg Loss: 3.0885
Finetune Epoch 10 | Batch 100/629 | Avg Loss: 3.1246
Finetune Epoch 10 | Batch 150/629 | Avg Loss: 3.1268
Finetune Epoch 10 | Batch 200/629 | Avg Loss: 3.1510
Finetune Epoch 10 | Batch 250/629 | Avg Loss: 3.1580
Finetune Epoch 10 | Batch 300/629 | Avg Loss: 3.1542
Finetune Epoch 10 | Batch 350/629 | Avg Loss: 3.1506
Finetune Epoch 10 | Batch 400/629 | Avg Loss: 3.1516
Finetune Epoch 10 | Batch 450/629 | Avg Loss: 3.1499
Finetune Epoch 10 | Batch 500/629 | Avg Loss: 3.1448
Finetune Epoch 10 | Batch 550/629 | Avg Loss: 3.1420
Finetune Epoch 10 | Batch 600/629 | Avg Loss: 3.1399
--- End of Finetune Epoch 10 | Train Loss: 3.1419 | Val Loss: 3.4154 | Val Acc: 0.0071 | Val F1: 0.1800 | LR: 0.000005 ---
No improvement for 3 epoch(s).

Early stopping triggered after 3 epochs without improvement.
Best validation loss: 3.4035 | Best F1: 0.1764

Fine-tuning finished.
Best fine-tuned QA model state_dict saved to models/SmallQA_20251214-214625/toy_llm_qasrl_finetuned.pth
Loading best model from models/SmallQA_20251214-214625/toy_llm_qasrl_finetuned.pth for final stats.
Final Validation Accuracy: 0.0101 | Final F1: 0.1764
Final training statistics saved to 'models/SmallQA_20251214-214625/finetune_stats.json'
============================================
Job completed: Sun 14 Dec 23:28:33 GMT 2025
============================================
