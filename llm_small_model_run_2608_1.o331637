Tue Aug 26 17:57:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:58:00.0 Off |                    0 |
| N/A   33C    P0             37W /  250W |       1MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
--- Loaded configuration 'tiny' from '/home/zceccgr/Scratch/freezeLLM/llm_scale_up/config.json' ---
Using device: cuda
Loading standard tokenizer ('bert-base-uncased')...
Standard vocabulary size: 30522
PAD token ID: 0
Instantiated Base Model: TinyQA with 4,336,384 trainable parameters.
Best pre-trained model will be saved to: models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth

--- Starting CLM Pre-training on nq_open for 'TinyQA' model ---
--- End of Pre-train Epoch 1 | Train Loss: 6.1280 | Validation Loss: 5.6444 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 2 | Train Loss: 5.3188 | Validation Loss: 5.3802 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 3 | Train Loss: 5.0439 | Validation Loss: 5.2648 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 4 | Train Loss: 4.8761 | Validation Loss: 5.2073 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 5 | Train Loss: 4.7589 | Validation Loss: 5.1640 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 6 | Train Loss: 4.6714 | Validation Loss: 5.1400 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 7 | Train Loss: 4.6021 | Validation Loss: 5.1170 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 8 | Train Loss: 4.5460 | Validation Loss: 5.1079 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 9 | Train Loss: 4.4961 | Validation Loss: 5.1095 | LR: 0.000299 ---
--- End of Pre-train Epoch 10 | Train Loss: 4.4548 | Validation Loss: 5.1089 | LR: 0.000299 ---
--- End of Pre-train Epoch 11 | Train Loss: 4.4175 | Validation Loss: 5.0999 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 12 | Train Loss: 4.3869 | Validation Loss: 5.0851 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 13 | Train Loss: 4.3591 | Validation Loss: 5.0903 | LR: 0.000299 ---
--- End of Pre-train Epoch 14 | Train Loss: 4.3349 | Validation Loss: 5.0844 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 15 | Train Loss: 4.3129 | Validation Loss: 5.0815 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 16 | Train Loss: 4.2947 | Validation Loss: 5.0841 | LR: 0.000299 ---
--- End of Pre-train Epoch 17 | Train Loss: 4.2777 | Validation Loss: 5.0832 | LR: 0.000299 ---
--- End of Pre-train Epoch 18 | Train Loss: 4.2637 | Validation Loss: 5.1024 | LR: 0.000299 ---
--- End of Pre-train Epoch 19 | Train Loss: 4.0827 | Validation Loss: 5.0524 | LR: 0.000150 ---
Validation loss improved. Saving model to models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 20 | Train Loss: 4.0379 | Validation Loss: 5.0691 | LR: 0.000150 ---
--- End of Pre-train Epoch 21 | Train Loss: 4.0237 | Validation Loss: 5.0665 | LR: 0.000150 ---
--- End of Pre-train Epoch 22 | Train Loss: 4.0140 | Validation Loss: 5.0714 | LR: 0.000150 ---
--- End of Pre-train Epoch 23 | Train Loss: 3.9030 | Validation Loss: 5.0587 | LR: 0.000075 ---
--- End of Pre-train Epoch 24 | Train Loss: 3.8827 | Validation Loss: 5.0610 | LR: 0.000075 ---

Early stopping triggered after 5 epochs without improvement.

Loading best pretrained model from models/TinyQA_20250826-175753/toy_llm_unified_pretrained.pth

Testing prompt completion after pretraining:
Prompt: 'The capital of France is'
Model completion: the capital of france is ambassador to the united states cristiano ronaldo

--- Starting Fine-tuning on QA-SRL for 'TinyQA' model ---
Successfully loaded pre-trained weights into the QA model.
Loading and processing qa_srl dataset for 'train' split...
Finished processing 'train'. Found 7562 valid QA examples.
Starting fine-tuning for 6 epochs...
Finetune Epoch 1 | Batch 50/946 | Avg Loss: 4.5327
Finetune Epoch 1 | Batch 100/946 | Avg Loss: 4.1874
Finetune Epoch 1 | Batch 150/946 | Avg Loss: 3.9601
Finetune Epoch 1 | Batch 200/946 | Avg Loss: 3.8269
Finetune Epoch 1 | Batch 250/946 | Avg Loss: 3.7168
Finetune Epoch 1 | Batch 300/946 | Avg Loss: 3.6340
Finetune Epoch 1 | Batch 350/946 | Avg Loss: 3.5730
Finetune Epoch 1 | Batch 400/946 | Avg Loss: 3.5153
Finetune Epoch 1 | Batch 450/946 | Avg Loss: 3.4693
Finetune Epoch 1 | Batch 500/946 | Avg Loss: 3.4235
Finetune Epoch 1 | Batch 550/946 | Avg Loss: 3.3937
Finetune Epoch 1 | Batch 600/946 | Avg Loss: 3.3557
Finetune Epoch 1 | Batch 650/946 | Avg Loss: 3.3234
Finetune Epoch 1 | Batch 700/946 | Avg Loss: 3.2986
Finetune Epoch 1 | Batch 750/946 | Avg Loss: 3.2768
Finetune Epoch 1 | Batch 800/946 | Avg Loss: 3.2557
Finetune Epoch 1 | Batch 850/946 | Avg Loss: 3.2333
Finetune Epoch 1 | Batch 900/946 | Avg Loss: 3.2144
--- End of Finetune Epoch 1 | Average QA Loss: 3.1979 | LR: 0.000050 ---
Finetune Epoch 2 | Batch 50/946 | Avg Loss: 2.6870
Finetune Epoch 2 | Batch 100/946 | Avg Loss: 2.6732
Finetune Epoch 2 | Batch 150/946 | Avg Loss: 2.6601
Finetune Epoch 2 | Batch 200/946 | Avg Loss: 2.6681
Finetune Epoch 2 | Batch 250/946 | Avg Loss: 2.6795
Finetune Epoch 2 | Batch 300/946 | Avg Loss: 2.6817
Finetune Epoch 2 | Batch 350/946 | Avg Loss: 2.6889
Finetune Epoch 2 | Batch 400/946 | Avg Loss: 2.6816
Finetune Epoch 2 | Batch 450/946 | Avg Loss: 2.6769
Finetune Epoch 2 | Batch 500/946 | Avg Loss: 2.6714
Finetune Epoch 2 | Batch 550/946 | Avg Loss: 2.6639
Finetune Epoch 2 | Batch 600/946 | Avg Loss: 2.6604
Finetune Epoch 2 | Batch 650/946 | Avg Loss: 2.6546
Finetune Epoch 2 | Batch 700/946 | Avg Loss: 2.6505
Finetune Epoch 2 | Batch 750/946 | Avg Loss: 2.6463
Finetune Epoch 2 | Batch 800/946 | Avg Loss: 2.6396
Finetune Epoch 2 | Batch 850/946 | Avg Loss: 2.6387
Finetune Epoch 2 | Batch 900/946 | Avg Loss: 2.6343
--- End of Finetune Epoch 2 | Average QA Loss: 2.6336 | LR: 0.000050 ---
Finetune Epoch 3 | Batch 50/946 | Avg Loss: 2.3963
Finetune Epoch 3 | Batch 100/946 | Avg Loss: 2.3993
Finetune Epoch 3 | Batch 150/946 | Avg Loss: 2.3777
Finetune Epoch 3 | Batch 200/946 | Avg Loss: 2.3919
Finetune Epoch 3 | Batch 250/946 | Avg Loss: 2.3881
Finetune Epoch 3 | Batch 300/946 | Avg Loss: 2.4004
Finetune Epoch 3 | Batch 350/946 | Avg Loss: 2.4108
Finetune Epoch 3 | Batch 400/946 | Avg Loss: 2.4106
Finetune Epoch 3 | Batch 450/946 | Avg Loss: 2.4043
Finetune Epoch 3 | Batch 500/946 | Avg Loss: 2.3976
Finetune Epoch 3 | Batch 550/946 | Avg Loss: 2.3992
Finetune Epoch 3 | Batch 600/946 | Avg Loss: 2.3917
Finetune Epoch 3 | Batch 650/946 | Avg Loss: 2.3920
Finetune Epoch 3 | Batch 700/946 | Avg Loss: 2.3887
Finetune Epoch 3 | Batch 750/946 | Avg Loss: 2.3890
Finetune Epoch 3 | Batch 800/946 | Avg Loss: 2.3821
Finetune Epoch 3 | Batch 850/946 | Avg Loss: 2.3798
Finetune Epoch 3 | Batch 900/946 | Avg Loss: 2.3776
--- End of Finetune Epoch 3 | Average QA Loss: 2.3731 | LR: 0.000050 ---
Finetune Epoch 4 | Batch 50/946 | Avg Loss: 2.2091
Finetune Epoch 4 | Batch 100/946 | Avg Loss: 2.1578
Finetune Epoch 4 | Batch 150/946 | Avg Loss: 2.1702
Finetune Epoch 4 | Batch 200/946 | Avg Loss: 2.1606
Finetune Epoch 4 | Batch 250/946 | Avg Loss: 2.1630
Finetune Epoch 4 | Batch 300/946 | Avg Loss: 2.1772
Finetune Epoch 4 | Batch 350/946 | Avg Loss: 2.1809
Finetune Epoch 4 | Batch 400/946 | Avg Loss: 2.1876
Finetune Epoch 4 | Batch 450/946 | Avg Loss: 2.1767
Finetune Epoch 4 | Batch 500/946 | Avg Loss: 2.1773
Finetune Epoch 4 | Batch 550/946 | Avg Loss: 2.1727
Finetune Epoch 4 | Batch 600/946 | Avg Loss: 2.1746
Finetune Epoch 4 | Batch 650/946 | Avg Loss: 2.1746
Finetune Epoch 4 | Batch 700/946 | Avg Loss: 2.1779
Finetune Epoch 4 | Batch 750/946 | Avg Loss: 2.1788
Finetune Epoch 4 | Batch 800/946 | Avg Loss: 2.1748
Finetune Epoch 4 | Batch 850/946 | Avg Loss: 2.1737
Finetune Epoch 4 | Batch 900/946 | Avg Loss: 2.1725
--- End of Finetune Epoch 4 | Average QA Loss: 2.1732 | LR: 0.000050 ---
Finetune Epoch 5 | Batch 50/946 | Avg Loss: 1.9989
Finetune Epoch 5 | Batch 100/946 | Avg Loss: 1.9823
Finetune Epoch 5 | Batch 150/946 | Avg Loss: 1.9695
Finetune Epoch 5 | Batch 200/946 | Avg Loss: 1.9830
Finetune Epoch 5 | Batch 250/946 | Avg Loss: 2.0011
Finetune Epoch 5 | Batch 300/946 | Avg Loss: 1.9991
Finetune Epoch 5 | Batch 350/946 | Avg Loss: 1.9883
Finetune Epoch 5 | Batch 400/946 | Avg Loss: 1.9951
Finetune Epoch 5 | Batch 450/946 | Avg Loss: 1.9987
Finetune Epoch 5 | Batch 500/946 | Avg Loss: 2.0064
Finetune Epoch 5 | Batch 550/946 | Avg Loss: 1.9984
Finetune Epoch 5 | Batch 600/946 | Avg Loss: 1.9966
Finetune Epoch 5 | Batch 650/946 | Avg Loss: 2.0017
Finetune Epoch 5 | Batch 700/946 | Avg Loss: 2.0080
Finetune Epoch 5 | Batch 750/946 | Avg Loss: 2.0114
Finetune Epoch 5 | Batch 800/946 | Avg Loss: 2.0105
Finetune Epoch 5 | Batch 850/946 | Avg Loss: 2.0138
Finetune Epoch 5 | Batch 900/946 | Avg Loss: 2.0138
--- End of Finetune Epoch 5 | Average QA Loss: 2.0162 | LR: 0.000050 ---
Finetune Epoch 6 | Batch 50/946 | Avg Loss: 1.8904
Finetune Epoch 6 | Batch 100/946 | Avg Loss: 1.8727
Finetune Epoch 6 | Batch 150/946 | Avg Loss: 1.8534
Finetune Epoch 6 | Batch 200/946 | Avg Loss: 1.8455
Finetune Epoch 6 | Batch 250/946 | Avg Loss: 1.8482
Finetune Epoch 6 | Batch 300/946 | Avg Loss: 1.8403
Finetune Epoch 6 | Batch 350/946 | Avg Loss: 1.8337
Finetune Epoch 6 | Batch 400/946 | Avg Loss: 1.8423
Finetune Epoch 6 | Batch 450/946 | Avg Loss: 1.8443
Finetune Epoch 6 | Batch 500/946 | Avg Loss: 1.8516
Finetune Epoch 6 | Batch 550/946 | Avg Loss: 1.8554
Finetune Epoch 6 | Batch 600/946 | Avg Loss: 1.8579
Finetune Epoch 6 | Batch 650/946 | Avg Loss: 1.8562
Finetune Epoch 6 | Batch 700/946 | Avg Loss: 1.8529
Finetune Epoch 6 | Batch 750/946 | Avg Loss: 1.8585
Finetune Epoch 6 | Batch 800/946 | Avg Loss: 1.8640
Finetune Epoch 6 | Batch 850/946 | Avg Loss: 1.8663
Finetune Epoch 6 | Batch 900/946 | Avg Loss: 1.8655
--- End of Finetune Epoch 6 | Average QA Loss: 1.8696 | LR: 0.000050 ---

Fine-tuning finished.
Fine-tuned QA model state_dict saved to models/TinyQA_20250826-175753/toy_llm_qasrl_finetuned.pth
Final Training Set Accuracy: 0.3067 | Final F1: 0.4845
Final training statistics saved to 'models/TinyQA_20250826-175753/finetune_stats.json'

--- Running Validation Metrics on QA-SRL Validation Set for 'TinyQA' model ---
Loaded finetuned model from models/TinyQA_20250826-175753/toy_llm_qasrl_finetuned.pth for validation.
Loading and processing qa_srl dataset for 'validation' split...
Finished processing 'validation'. Found 2571 valid QA examples.
Final Validation Accuracy: 0.1042 | Final Validation F1: 0.2716
Validation statistics saved to 'models/TinyQA_20250826-175753/validation_stats.json'
