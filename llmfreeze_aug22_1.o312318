Fri Aug 22 19:12:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:D8:00.0 Off |                    0 |
| N/A   32C    P0             36W /  250W |       1MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Using device: cuda
Loading standard tokenizer ('bert-base-uncased')...
Standard vocabulary size: 30522
PAD token ID: 0
Instantiated Base Model: TinyQA with 4,336,384 trainable parameters.
Best pre-trained model will be saved to: models/date_20250822-191227/toy_llm_unified_pretrained.pth

--- Starting CLM Pre-training on nq_open ---
--- End of Pre-train Epoch 1 | Train Loss: 6.1252 | Validation Loss: 5.6448 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 2 | Train Loss: 5.3161 | Validation Loss: 5.3816 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 3 | Train Loss: 5.0458 | Validation Loss: 5.2685 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 4 | Train Loss: 4.8795 | Validation Loss: 5.2134 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 5 | Train Loss: 4.7622 | Validation Loss: 5.1751 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 6 | Train Loss: 4.6744 | Validation Loss: 5.1561 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 7 | Train Loss: 4.6057 | Validation Loss: 5.1281 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 8 | Train Loss: 4.5485 | Validation Loss: 5.1098 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 9 | Train Loss: 4.5002 | Validation Loss: 5.1001 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 10 | Train Loss: 4.4608 | Validation Loss: 5.1014 | LR: 0.000299 ---
--- End of Pre-train Epoch 11 | Train Loss: 4.4240 | Validation Loss: 5.0923 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 12 | Train Loss: 4.3928 | Validation Loss: 5.0863 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 13 | Train Loss: 4.3643 | Validation Loss: 5.0760 | LR: 0.000299 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 14 | Train Loss: 4.3399 | Validation Loss: 5.0828 | LR: 0.000299 ---
--- End of Pre-train Epoch 15 | Train Loss: 4.3204 | Validation Loss: 5.0813 | LR: 0.000299 ---
--- End of Pre-train Epoch 16 | Train Loss: 4.2995 | Validation Loss: 5.0848 | LR: 0.000299 ---
--- End of Pre-train Epoch 17 | Train Loss: 4.1174 | Validation Loss: 5.0434 | LR: 0.000150 ---
Validation loss improved. Saving model to models/date_20250822-191227/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 18 | Train Loss: 4.0713 | Validation Loss: 5.0504 | LR: 0.000150 ---
--- End of Pre-train Epoch 19 | Train Loss: 4.0543 | Validation Loss: 5.0512 | LR: 0.000150 ---
--- End of Pre-train Epoch 20 | Train Loss: 4.0444 | Validation Loss: 5.0555 | LR: 0.000150 ---
--- End of Pre-train Epoch 21 | Train Loss: 3.9323 | Validation Loss: 5.0512 | LR: 0.000075 ---
--- End of Pre-train Epoch 22 | Train Loss: 3.9098 | Validation Loss: 5.0551 | LR: 0.000075 ---

Early stopping triggered after 5 epochs without improvement.

Loading best pretrained model from models/date_20250822-191227/toy_llm_unified_pretrained.pth

Testing prompt completion after pretraining:
Prompt: 'The capital of France is'
Model completion: the capital of france is the zu con peru of world alasta rural valley

--- Starting Fine-tuning on QA-SRL ---
Successfully loaded pre-trained weights into the QA model.
Loading and processing qa_srl dataset for 'train' split...
Finished processing 'train'. Found 7562 valid QA examples.
Starting fine-tuning for 6 epochs...
Finetune Epoch 1 | Batch 50/946 | Avg Loss: 4.4725
Finetune Epoch 1 | Batch 100/946 | Avg Loss: 4.0992
Finetune Epoch 1 | Batch 150/946 | Avg Loss: 3.9123
Finetune Epoch 1 | Batch 200/946 | Avg Loss: 3.7538
Finetune Epoch 1 | Batch 250/946 | Avg Loss: 3.6591
Finetune Epoch 1 | Batch 300/946 | Avg Loss: 3.5902
Finetune Epoch 1 | Batch 350/946 | Avg Loss: 3.5248
Finetune Epoch 1 | Batch 400/946 | Avg Loss: 3.4769
Finetune Epoch 1 | Batch 450/946 | Avg Loss: 3.4279
Finetune Epoch 1 | Batch 500/946 | Avg Loss: 3.3931
Finetune Epoch 1 | Batch 550/946 | Avg Loss: 3.3609
Finetune Epoch 1 | Batch 600/946 | Avg Loss: 3.3324
Finetune Epoch 1 | Batch 650/946 | Avg Loss: 3.3054
Finetune Epoch 1 | Batch 700/946 | Avg Loss: 3.2831
Finetune Epoch 1 | Batch 750/946 | Avg Loss: 3.2492
Finetune Epoch 1 | Batch 800/946 | Avg Loss: 3.2188
Finetune Epoch 1 | Batch 850/946 | Avg Loss: 3.2052
Finetune Epoch 1 | Batch 900/946 | Avg Loss: 3.1899
--- End of Finetune Epoch 1 | Average QA Loss: 3.1698 | LR: 0.000050 ---
Finetune Epoch 2 | Batch 50/946 | Avg Loss: 2.7178
Finetune Epoch 2 | Batch 100/946 | Avg Loss: 2.6957
Finetune Epoch 2 | Batch 150/946 | Avg Loss: 2.6900
Finetune Epoch 2 | Batch 200/946 | Avg Loss: 2.6774
Finetune Epoch 2 | Batch 250/946 | Avg Loss: 2.6588
Finetune Epoch 2 | Batch 300/946 | Avg Loss: 2.6558
Finetune Epoch 2 | Batch 350/946 | Avg Loss: 2.6530
Finetune Epoch 2 | Batch 400/946 | Avg Loss: 2.6486
Finetune Epoch 2 | Batch 450/946 | Avg Loss: 2.6473
Finetune Epoch 2 | Batch 500/946 | Avg Loss: 2.6447
Finetune Epoch 2 | Batch 550/946 | Avg Loss: 2.6402
Finetune Epoch 2 | Batch 600/946 | Avg Loss: 2.6354
Finetune Epoch 2 | Batch 650/946 | Avg Loss: 2.6292
Finetune Epoch 2 | Batch 700/946 | Avg Loss: 2.6293
Finetune Epoch 2 | Batch 750/946 | Avg Loss: 2.6317
Finetune Epoch 2 | Batch 800/946 | Avg Loss: 2.6202
Finetune Epoch 2 | Batch 850/946 | Avg Loss: 2.6162
Finetune Epoch 2 | Batch 900/946 | Avg Loss: 2.6163
--- End of Finetune Epoch 2 | Average QA Loss: 2.6160 | LR: 0.000050 ---
Finetune Epoch 3 | Batch 50/946 | Avg Loss: 2.3262
Finetune Epoch 3 | Batch 100/946 | Avg Loss: 2.3559
Finetune Epoch 3 | Batch 150/946 | Avg Loss: 2.3594
Finetune Epoch 3 | Batch 200/946 | Avg Loss: 2.3733
Finetune Epoch 3 | Batch 250/946 | Avg Loss: 2.3638
Finetune Epoch 3 | Batch 300/946 | Avg Loss: 2.3737
Finetune Epoch 3 | Batch 350/946 | Avg Loss: 2.3593
Finetune Epoch 3 | Batch 400/946 | Avg Loss: 2.3607
Finetune Epoch 3 | Batch 450/946 | Avg Loss: 2.3647
Finetune Epoch 3 | Batch 500/946 | Avg Loss: 2.3674
Finetune Epoch 3 | Batch 550/946 | Avg Loss: 2.3620
Finetune Epoch 3 | Batch 600/946 | Avg Loss: 2.3640
Finetune Epoch 3 | Batch 650/946 | Avg Loss: 2.3610
Finetune Epoch 3 | Batch 700/946 | Avg Loss: 2.3626
Finetune Epoch 3 | Batch 750/946 | Avg Loss: 2.3609
Finetune Epoch 3 | Batch 800/946 | Avg Loss: 2.3623
Finetune Epoch 3 | Batch 850/946 | Avg Loss: 2.3610
Finetune Epoch 3 | Batch 900/946 | Avg Loss: 2.3643
--- End of Finetune Epoch 3 | Average QA Loss: 2.3618 | LR: 0.000050 ---
Finetune Epoch 4 | Batch 50/946 | Avg Loss: 2.1655
Finetune Epoch 4 | Batch 100/946 | Avg Loss: 2.1313
Finetune Epoch 4 | Batch 150/946 | Avg Loss: 2.1271
Finetune Epoch 4 | Batch 200/946 | Avg Loss: 2.1238
Finetune Epoch 4 | Batch 250/946 | Avg Loss: 2.1473
Finetune Epoch 4 | Batch 300/946 | Avg Loss: 2.1491
Finetune Epoch 4 | Batch 350/946 | Avg Loss: 2.1514
Finetune Epoch 4 | Batch 400/946 | Avg Loss: 2.1563
Finetune Epoch 4 | Batch 450/946 | Avg Loss: 2.1539
Finetune Epoch 4 | Batch 500/946 | Avg Loss: 2.1511
Finetune Epoch 4 | Batch 550/946 | Avg Loss: 2.1532
Finetune Epoch 4 | Batch 600/946 | Avg Loss: 2.1573
Finetune Epoch 4 | Batch 650/946 | Avg Loss: 2.1617
Finetune Epoch 4 | Batch 700/946 | Avg Loss: 2.1627
Finetune Epoch 4 | Batch 750/946 | Avg Loss: 2.1654
Finetune Epoch 4 | Batch 800/946 | Avg Loss: 2.1621
Finetune Epoch 4 | Batch 850/946 | Avg Loss: 2.1627
Finetune Epoch 4 | Batch 900/946 | Avg Loss: 2.1633
--- End of Finetune Epoch 4 | Average QA Loss: 2.1644 | LR: 0.000050 ---
Finetune Epoch 5 | Batch 50/946 | Avg Loss: 1.9401
Finetune Epoch 5 | Batch 100/946 | Avg Loss: 1.9585
Finetune Epoch 5 | Batch 150/946 | Avg Loss: 1.9629
Finetune Epoch 5 | Batch 200/946 | Avg Loss: 1.9649
Finetune Epoch 5 | Batch 250/946 | Avg Loss: 1.9764
Finetune Epoch 5 | Batch 300/946 | Avg Loss: 1.9895
Finetune Epoch 5 | Batch 350/946 | Avg Loss: 1.9909
Finetune Epoch 5 | Batch 400/946 | Avg Loss: 1.9962
Finetune Epoch 5 | Batch 450/946 | Avg Loss: 1.9959
Finetune Epoch 5 | Batch 500/946 | Avg Loss: 2.0044
Finetune Epoch 5 | Batch 550/946 | Avg Loss: 2.0042
Finetune Epoch 5 | Batch 600/946 | Avg Loss: 2.0009
Finetune Epoch 5 | Batch 650/946 | Avg Loss: 2.0027
Finetune Epoch 5 | Batch 700/946 | Avg Loss: 1.9993
Finetune Epoch 5 | Batch 750/946 | Avg Loss: 2.0047
Finetune Epoch 5 | Batch 800/946 | Avg Loss: 2.0069
Finetune Epoch 5 | Batch 850/946 | Avg Loss: 2.0060
Finetune Epoch 5 | Batch 900/946 | Avg Loss: 2.0109
--- End of Finetune Epoch 5 | Average QA Loss: 2.0130 | LR: 0.000050 ---
Finetune Epoch 6 | Batch 50/946 | Avg Loss: 1.8975
Finetune Epoch 6 | Batch 100/946 | Avg Loss: 1.8552
Finetune Epoch 6 | Batch 150/946 | Avg Loss: 1.8527
Finetune Epoch 6 | Batch 200/946 | Avg Loss: 1.8479
Finetune Epoch 6 | Batch 250/946 | Avg Loss: 1.8554
Finetune Epoch 6 | Batch 300/946 | Avg Loss: 1.8656
Finetune Epoch 6 | Batch 350/946 | Avg Loss: 1.8579
Finetune Epoch 6 | Batch 400/946 | Avg Loss: 1.8616
Finetune Epoch 6 | Batch 450/946 | Avg Loss: 1.8655
Finetune Epoch 6 | Batch 500/946 | Avg Loss: 1.8702
Finetune Epoch 6 | Batch 550/946 | Avg Loss: 1.8710
Finetune Epoch 6 | Batch 600/946 | Avg Loss: 1.8667
Finetune Epoch 6 | Batch 650/946 | Avg Loss: 1.8676
Finetune Epoch 6 | Batch 700/946 | Avg Loss: 1.8729
Finetune Epoch 6 | Batch 750/946 | Avg Loss: 1.8706
Finetune Epoch 6 | Batch 800/946 | Avg Loss: 1.8728
Finetune Epoch 6 | Batch 850/946 | Avg Loss: 1.8755
Finetune Epoch 6 | Batch 900/946 | Avg Loss: 1.8827
--- End of Finetune Epoch 6 | Average QA Loss: 1.8865 | LR: 0.000050 ---

Fine-tuning finished.
Fine-tuned QA model state_dict saved to models/date_20250822-191227/toy_llm_qasrl_finetuned.pth
Final Training Set Accuracy: 0.2736 | Final F1: 0.4616
Final training statistics saved to 'models/date_20250822-191227/finetune_stats.json'

--- Running Validation Metrics on QA-SRL Validation Set ---
Loaded finetuned model from models/date_20250822-191227/toy_llm_qasrl_finetuned.pth for validation.
Loading and processing qa_srl dataset for 'validation' split...
Finished processing 'validation'. Found 2571 valid QA examples.
Final Validation Accuracy: 0.0972 | Final Validation F1: 0.2767
Validation statistics saved to 'models/date_20250822-191227/validation_stats.json'
