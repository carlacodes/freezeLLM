============================================
Starting job: BaseQA_1214
Model size: base
Date: Sun 14 Dec 18:43:44 GMT 2025
============================================
Sun Dec 14 18:43:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          On  |   00000000:06:00.0 Off |                    0 |
| N/A   32C    P0             33W /  250W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Starting Python script for BaseQA model...
--- Loaded configuration 'base' from '/home/zceccgr/Scratch/freezeLLM/llm_scale_up/config.json' ---
Using device: cuda
Loading standard tokenizer ('bert-base-uncased')...
Standard vocabulary size: 30522
PAD token ID: 0
Instantiated Base Model: BaseQA with 34,804,736 trainable parameters.
Best pre-trained model will be saved to: models/BaseQA_20251214-184412/toy_llm_unified_pretrained.pth

--- Starting CLM Pre-training on nq_open for 'BaseQA' model ---
Pre-training batch size: 8 | Gradient accumulation: 4 | Effective batch size: 32
--- End of Pre-train Epoch 1 | Train Loss: 5.0663 | Validation Loss: 4.6914 | LR: 0.000100 ---
Validation loss improved. Saving model to models/BaseQA_20251214-184412/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 2 | Train Loss: 4.3855 | Validation Loss: 4.4455 | LR: 0.000100 ---
Validation loss improved. Saving model to models/BaseQA_20251214-184412/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 3 | Train Loss: 4.1023 | Validation Loss: 4.3177 | LR: 0.000100 ---
Validation loss improved. Saving model to models/BaseQA_20251214-184412/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 4 | Train Loss: 3.8849 | Validation Loss: 4.2383 | LR: 0.000100 ---
Validation loss improved. Saving model to models/BaseQA_20251214-184412/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 5 | Train Loss: 3.7026 | Validation Loss: 4.1933 | LR: 0.000100 ---
Validation loss improved. Saving model to models/BaseQA_20251214-184412/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 6 | Train Loss: 3.5420 | Validation Loss: 4.1737 | LR: 0.000100 ---
Validation loss improved. Saving model to models/BaseQA_20251214-184412/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 7 | Train Loss: 3.3980 | Validation Loss: 4.1618 | LR: 0.000100 ---
Validation loss improved. Saving model to models/BaseQA_20251214-184412/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 8 | Train Loss: 3.2661 | Validation Loss: 4.1667 | LR: 0.000100 ---
--- End of Pre-train Epoch 9 | Train Loss: 3.1462 | Validation Loss: 4.1901 | LR: 0.000100 ---
--- End of Pre-train Epoch 10 | Train Loss: 3.0368 | Validation Loss: 4.2092 | LR: 0.000100 ---
--- End of Pre-train Epoch 11 | Train Loss: 2.8582 | Validation Loss: 4.1986 | LR: 0.000050 ---
--- End of Pre-train Epoch 12 | Train Loss: 2.7749 | Validation Loss: 4.2202 | LR: 0.000050 ---

Early stopping triggered after 5 epochs without improvement.

Loading best pretrained model from models/BaseQA_20251214-184412/toy_llm_unified_pretrained.pth

Testing prompt completion after pretraining:
Prompt: 'Question: where was the statue of liberty originally built'
Model completion: question : where was the statue of liberty originally built from france to the usa answer : liberty national park

--- Starting Fine-tuning on QA-SRL for 'BaseQA' model ---
Successfully loaded pre-trained weights into the QA model.
Loading and processing qa_srl dataset for 'train' split...
Finished processing 'train'. Found 5031 valid QA examples.
Loading and processing qa_srl dataset for 'validation' split...
Finished processing 'validation'. Found 1686 valid QA examples.
Starting fine-tuning for up to 20 epochs...
Batch size: 8 | Gradient accumulation steps: 4 | Effective batch size: 32
Early stopping patience: 3 epochs
Finetune Epoch 1 | Batch 50/629 | Avg Loss: 5.8200
Finetune Epoch 1 | Batch 100/629 | Avg Loss: 5.4426
Finetune Epoch 1 | Batch 150/629 | Avg Loss: 5.1500
Finetune Epoch 1 | Batch 200/629 | Avg Loss: 4.9865
Finetune Epoch 1 | Batch 250/629 | Avg Loss: 4.8389
Finetune Epoch 1 | Batch 300/629 | Avg Loss: 4.7550
Finetune Epoch 1 | Batch 350/629 | Avg Loss: 4.6718
Finetune Epoch 1 | Batch 400/629 | Avg Loss: 4.5982
Finetune Epoch 1 | Batch 450/629 | Avg Loss: 4.5202
Finetune Epoch 1 | Batch 500/629 | Avg Loss: 4.4561
Finetune Epoch 1 | Batch 550/629 | Avg Loss: 4.3958
Finetune Epoch 1 | Batch 600/629 | Avg Loss: 4.3468
--- End of Finetune Epoch 1 | Train Loss: 4.3307 | Val Loss: 3.6530 | Val Acc: 0.0042 | Val F1: 0.0962 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/BaseQA_20251214-184412/toy_llm_qasrl_finetuned.pth
Finetune Epoch 2 | Batch 50/629 | Avg Loss: 3.6216
Finetune Epoch 2 | Batch 100/629 | Avg Loss: 3.6465
Finetune Epoch 2 | Batch 150/629 | Avg Loss: 3.6489
Finetune Epoch 2 | Batch 200/629 | Avg Loss: 3.6320
Finetune Epoch 2 | Batch 250/629 | Avg Loss: 3.6029
Finetune Epoch 2 | Batch 300/629 | Avg Loss: 3.5953
Finetune Epoch 2 | Batch 350/629 | Avg Loss: 3.5813
Finetune Epoch 2 | Batch 400/629 | Avg Loss: 3.5848
Finetune Epoch 2 | Batch 450/629 | Avg Loss: 3.5693
Finetune Epoch 2 | Batch 500/629 | Avg Loss: 3.5598
Finetune Epoch 2 | Batch 550/629 | Avg Loss: 3.5572
Finetune Epoch 2 | Batch 600/629 | Avg Loss: 3.5492
--- End of Finetune Epoch 2 | Train Loss: 3.5454 | Val Loss: 3.5130 | Val Acc: 0.0101 | Val F1: 0.1269 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/BaseQA_20251214-184412/toy_llm_qasrl_finetuned.pth
Finetune Epoch 3 | Batch 50/629 | Avg Loss: 3.3658
Finetune Epoch 3 | Batch 100/629 | Avg Loss: 3.3480
Finetune Epoch 3 | Batch 150/629 | Avg Loss: 3.3683
Finetune Epoch 3 | Batch 200/629 | Avg Loss: 3.3724
Finetune Epoch 3 | Batch 250/629 | Avg Loss: 3.3741
Finetune Epoch 3 | Batch 300/629 | Avg Loss: 3.3664
Finetune Epoch 3 | Batch 350/629 | Avg Loss: 3.3694
Finetune Epoch 3 | Batch 400/629 | Avg Loss: 3.3642
Finetune Epoch 3 | Batch 450/629 | Avg Loss: 3.3672
Finetune Epoch 3 | Batch 500/629 | Avg Loss: 3.3599
Finetune Epoch 3 | Batch 550/629 | Avg Loss: 3.3577
Finetune Epoch 3 | Batch 600/629 | Avg Loss: 3.3548
--- End of Finetune Epoch 3 | Train Loss: 3.3512 | Val Loss: 3.4548 | Val Acc: 0.0113 | Val F1: 0.1534 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/BaseQA_20251214-184412/toy_llm_qasrl_finetuned.pth
Finetune Epoch 4 | Batch 50/629 | Avg Loss: 3.2393
Finetune Epoch 4 | Batch 100/629 | Avg Loss: 3.2554
Finetune Epoch 4 | Batch 150/629 | Avg Loss: 3.2301
Finetune Epoch 4 | Batch 200/629 | Avg Loss: 3.2127
Finetune Epoch 4 | Batch 250/629 | Avg Loss: 3.2274
Finetune Epoch 4 | Batch 300/629 | Avg Loss: 3.2280
Finetune Epoch 4 | Batch 350/629 | Avg Loss: 3.2329
Finetune Epoch 4 | Batch 400/629 | Avg Loss: 3.2337
Finetune Epoch 4 | Batch 450/629 | Avg Loss: 3.2300
Finetune Epoch 4 | Batch 500/629 | Avg Loss: 3.2341
Finetune Epoch 4 | Batch 550/629 | Avg Loss: 3.2329
Finetune Epoch 4 | Batch 600/629 | Avg Loss: 3.2344
--- End of Finetune Epoch 4 | Train Loss: 3.2328 | Val Loss: 3.4167 | Val Acc: 0.0113 | Val F1: 0.1692 | LR: 0.000010 ---
Validation loss improved. Saving best model to models/BaseQA_20251214-184412/toy_llm_qasrl_finetuned.pth
Finetune Epoch 5 | Batch 50/629 | Avg Loss: 3.1384
Finetune Epoch 5 | Batch 100/629 | Avg Loss: 3.1374
Finetune Epoch 5 | Batch 150/629 | Avg Loss: 3.1484
Finetune Epoch 5 | Batch 200/629 | Avg Loss: 3.1307
Finetune Epoch 5 | Batch 250/629 | Avg Loss: 3.1302
Finetune Epoch 5 | Batch 300/629 | Avg Loss: 3.1473
Finetune Epoch 5 | Batch 350/629 | Avg Loss: 3.1436
Finetune Epoch 5 | Batch 400/629 | Avg Loss: 3.1451
Finetune Epoch 5 | Batch 450/629 | Avg Loss: 3.1468
Finetune Epoch 5 | Batch 500/629 | Avg Loss: 3.1454
Finetune Epoch 5 | Batch 550/629 | Avg Loss: 3.1451
Finetune Epoch 5 | Batch 600/629 | Avg Loss: 3.1464
--- End of Finetune Epoch 5 | Train Loss: 3.1409 | Val Loss: 3.4403 | Val Acc: 0.0125 | Val F1: 0.1764 | LR: 0.000010 ---
No improvement for 1 epoch(s).
Finetune Epoch 6 | Batch 50/629 | Avg Loss: 3.1182
Finetune Epoch 6 | Batch 100/629 | Avg Loss: 3.0693
Finetune Epoch 6 | Batch 150/629 | Avg Loss: 3.0480
Finetune Epoch 6 | Batch 200/629 | Avg Loss: 3.0441
Finetune Epoch 6 | Batch 250/629 | Avg Loss: 3.0425
Finetune Epoch 6 | Batch 300/629 | Avg Loss: 3.0374
Finetune Epoch 6 | Batch 350/629 | Avg Loss: 3.0387
Finetune Epoch 6 | Batch 400/629 | Avg Loss: 3.0418
Finetune Epoch 6 | Batch 450/629 | Avg Loss: 3.0416
Finetune Epoch 6 | Batch 500/629 | Avg Loss: 3.0439
Finetune Epoch 6 | Batch 550/629 | Avg Loss: 3.0407
Finetune Epoch 6 | Batch 600/629 | Avg Loss: 3.0405
--- End of Finetune Epoch 6 | Train Loss: 3.0393 | Val Loss: 3.5006 | Val Acc: 0.0136 | Val F1: 0.1863 | LR: 0.000010 ---
No improvement for 2 epoch(s).
Finetune Epoch 7 | Batch 50/629 | Avg Loss: 2.9971
Finetune Epoch 7 | Batch 100/629 | Avg Loss: 2.9707
Finetune Epoch 7 | Batch 150/629 | Avg Loss: 2.9528
Finetune Epoch 7 | Batch 200/629 | Avg Loss: 2.9526
Finetune Epoch 7 | Batch 250/629 | Avg Loss: 2.9577
Finetune Epoch 7 | Batch 300/629 | Avg Loss: 2.9510
Finetune Epoch 7 | Batch 350/629 | Avg Loss: 2.9534
Finetune Epoch 7 | Batch 400/629 | Avg Loss: 2.9501
Finetune Epoch 7 | Batch 450/629 | Avg Loss: 2.9411
Finetune Epoch 7 | Batch 500/629 | Avg Loss: 2.9390
Finetune Epoch 7 | Batch 550/629 | Avg Loss: 2.9389
Finetune Epoch 7 | Batch 600/629 | Avg Loss: 2.9447
--- End of Finetune Epoch 7 | Train Loss: 2.9455 | Val Loss: 3.4990 | Val Acc: 0.0190 | Val F1: 0.1856 | LR: 0.000005 ---
No improvement for 3 epoch(s).

Early stopping triggered after 3 epochs without improvement.
Best validation loss: 3.4167 | Best F1: 0.1692

Fine-tuning finished.
Best fine-tuned QA model state_dict saved to models/BaseQA_20251214-184412/toy_llm_qasrl_finetuned.pth
Loading best model from models/BaseQA_20251214-184412/toy_llm_qasrl_finetuned.pth for final stats.
Final Validation Accuracy: 0.0113 | Final F1: 0.1692
Final training statistics saved to 'models/BaseQA_20251214-184412/finetune_stats.json'
============================================
Job completed: Sun 14 Dec 22:08:15 GMT 2025
============================================
