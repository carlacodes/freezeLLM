Thu Sep  4 19:14:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:58:00.0 Off |                    0 |
| N/A   36C    P0             36W /  250W |       1MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
--- Loaded configuration 'tiny' from '/home/zceccgr/Scratch/freezeLLM/llm_scale_up/config.json' ---
Using device: cuda
Loading standard tokenizer ('bert-base-uncased')...
Standard vocabulary size: 30522
PAD token ID: 0
Instantiated Base Model: TinyQA with 4,336,384 trainable parameters.
Best pre-trained model will be saved to: models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth

--- Starting CLM Pre-training on nq_open for 'TinyQA' model ---
--- End of Pre-train Epoch 1 | Train Loss: 5.2001 | Validation Loss: 4.8085 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 2 | Train Loss: 4.5698 | Validation Loss: 4.5962 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 3 | Train Loss: 4.3692 | Validation Loss: 4.4835 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 4 | Train Loss: 4.2315 | Validation Loss: 4.4172 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 5 | Train Loss: 4.1305 | Validation Loss: 4.3733 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 6 | Train Loss: 4.0548 | Validation Loss: 4.3334 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 7 | Train Loss: 3.9975 | Validation Loss: 4.3094 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 8 | Train Loss: 3.9533 | Validation Loss: 4.3044 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 9 | Train Loss: 3.9188 | Validation Loss: 4.2899 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 10 | Train Loss: 3.8911 | Validation Loss: 4.2871 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 11 | Train Loss: 3.8675 | Validation Loss: 4.2861 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 12 | Train Loss: 3.8503 | Validation Loss: 4.2774 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 13 | Train Loss: 3.8337 | Validation Loss: 4.2737 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 14 | Train Loss: 3.8200 | Validation Loss: 4.2700 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 15 | Train Loss: 3.8081 | Validation Loss: 4.2721 | LR: 0.000299 ---
--- End of Pre-train Epoch 16 | Train Loss: 3.7988 | Validation Loss: 4.2703 | LR: 0.000299 ---
--- End of Pre-train Epoch 17 | Train Loss: 3.7908 | Validation Loss: 4.2716 | LR: 0.000299 ---
--- End of Pre-train Epoch 18 | Train Loss: 3.6773 | Validation Loss: 4.2424 | LR: 0.000150 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 19 | Train Loss: 3.6486 | Validation Loss: 4.2473 | LR: 0.000150 ---
--- End of Pre-train Epoch 20 | Train Loss: 3.6396 | Validation Loss: 4.2340 | LR: 0.000150 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 21 | Train Loss: 3.6334 | Validation Loss: 4.2468 | LR: 0.000150 ---
--- End of Pre-train Epoch 22 | Train Loss: 3.6299 | Validation Loss: 4.2427 | LR: 0.000150 ---
--- End of Pre-train Epoch 23 | Train Loss: 3.6276 | Validation Loss: 4.2417 | LR: 0.000150 ---
--- End of Pre-train Epoch 24 | Train Loss: 3.5552 | Validation Loss: 4.2335 | LR: 0.000075 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 25 | Train Loss: 3.5422 | Validation Loss: 4.2314 | LR: 0.000075 ---
Validation loss improved. Saving model to models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 26 | Train Loss: 3.5367 | Validation Loss: 4.2389 | LR: 0.000075 ---
--- End of Pre-train Epoch 27 | Train Loss: 3.5327 | Validation Loss: 4.2351 | LR: 0.000075 ---
--- End of Pre-train Epoch 28 | Train Loss: 3.5292 | Validation Loss: 4.2339 | LR: 0.000075 ---
--- End of Pre-train Epoch 29 | Train Loss: 3.4876 | Validation Loss: 4.2346 | LR: 0.000037 ---
--- End of Pre-train Epoch 30 | Train Loss: 3.4803 | Validation Loss: 4.2351 | LR: 0.000037 ---

Early stopping triggered after 5 epochs without improvement.

Loading best pretrained model from models/TinyQA_20250904-191429/toy_llm_unified_pretrained.pth

Testing prompt completion after pretraining:
Prompt: 'Question: where was the statue of liberty originally built'
Model completion: question : where was the statue of liberty originally built in 1903 answer : caribbean

--- Starting Fine-tuning on QA-SRL for 'TinyQA' model ---
Successfully loaded pre-trained weights into the QA model.
Loading and processing qa_srl dataset for 'train' split...
Finished processing 'train'. Found 7562 valid QA examples.
Loading and processing qa_srl dataset for 'validation' split...
Finished processing 'validation'. Found 2571 valid QA examples.
Starting fine-tuning for 6 epochs...
Finetune Epoch 1 | Batch 50/946 | Avg Loss: 4.8418
Finetune Epoch 1 | Batch 100/946 | Avg Loss: 4.4449
Finetune Epoch 1 | Batch 150/946 | Avg Loss: 4.1895
Finetune Epoch 1 | Batch 200/946 | Avg Loss: 4.0220
Finetune Epoch 1 | Batch 250/946 | Avg Loss: 3.8920
Finetune Epoch 1 | Batch 300/946 | Avg Loss: 3.7981
Finetune Epoch 1 | Batch 350/946 | Avg Loss: 3.7120
Finetune Epoch 1 | Batch 400/946 | Avg Loss: 3.6472
Finetune Epoch 1 | Batch 450/946 | Avg Loss: 3.5874
Finetune Epoch 1 | Batch 500/946 | Avg Loss: 3.5334
Finetune Epoch 1 | Batch 550/946 | Avg Loss: 3.4956
Finetune Epoch 1 | Batch 600/946 | Avg Loss: 3.4592
Finetune Epoch 1 | Batch 650/946 | Avg Loss: 3.4282
Finetune Epoch 1 | Batch 700/946 | Avg Loss: 3.4021
Finetune Epoch 1 | Batch 750/946 | Avg Loss: 3.3767
Finetune Epoch 1 | Batch 800/946 | Avg Loss: 3.3502
Finetune Epoch 1 | Batch 850/946 | Avg Loss: 3.3310
Finetune Epoch 1 | Batch 900/946 | Avg Loss: 3.3138
--- End of Finetune Epoch 1 | Train Loss: 3.2980 | Val Loss: 2.9309 | Val Acc: 0.0381 | Val F1: 0.1618 | LR: 0.000050 ---
Validation loss improved. Saving best model to models/TinyQA_20250904-191429/toy_llm_qasrl_finetuned.pth
Finetune Epoch 2 | Batch 50/946 | Avg Loss: 2.8901
Finetune Epoch 2 | Batch 100/946 | Avg Loss: 2.8694
Finetune Epoch 2 | Batch 150/946 | Avg Loss: 2.8425
Finetune Epoch 2 | Batch 200/946 | Avg Loss: 2.8222
Finetune Epoch 2 | Batch 250/946 | Avg Loss: 2.8061
Finetune Epoch 2 | Batch 300/946 | Avg Loss: 2.7829
Finetune Epoch 2 | Batch 350/946 | Avg Loss: 2.7739
Finetune Epoch 2 | Batch 400/946 | Avg Loss: 2.7648
Finetune Epoch 2 | Batch 450/946 | Avg Loss: 2.7695
Finetune Epoch 2 | Batch 500/946 | Avg Loss: 2.7680
Finetune Epoch 2 | Batch 550/946 | Avg Loss: 2.7656
Finetune Epoch 2 | Batch 600/946 | Avg Loss: 2.7581
Finetune Epoch 2 | Batch 650/946 | Avg Loss: 2.7526
Finetune Epoch 2 | Batch 700/946 | Avg Loss: 2.7471
Finetune Epoch 2 | Batch 750/946 | Avg Loss: 2.7400
Finetune Epoch 2 | Batch 800/946 | Avg Loss: 2.7379
Finetune Epoch 2 | Batch 850/946 | Avg Loss: 2.7363
Finetune Epoch 2 | Batch 900/946 | Avg Loss: 2.7355
--- End of Finetune Epoch 2 | Train Loss: 2.7314 | Val Loss: 2.8361 | Val Acc: 0.0727 | Val F1: 0.2149 | LR: 0.000050 ---
Validation loss improved. Saving best model to models/TinyQA_20250904-191429/toy_llm_qasrl_finetuned.pth
Finetune Epoch 3 | Batch 50/946 | Avg Loss: 2.5767
Finetune Epoch 3 | Batch 100/946 | Avg Loss: 2.5438
Finetune Epoch 3 | Batch 150/946 | Avg Loss: 2.5294
Finetune Epoch 3 | Batch 200/946 | Avg Loss: 2.5096
Finetune Epoch 3 | Batch 250/946 | Avg Loss: 2.4991
Finetune Epoch 3 | Batch 300/946 | Avg Loss: 2.4856
Finetune Epoch 3 | Batch 350/946 | Avg Loss: 2.4872
Finetune Epoch 3 | Batch 400/946 | Avg Loss: 2.4866
Finetune Epoch 3 | Batch 450/946 | Avg Loss: 2.4854
Finetune Epoch 3 | Batch 500/946 | Avg Loss: 2.4808
Finetune Epoch 3 | Batch 550/946 | Avg Loss: 2.4902
Finetune Epoch 3 | Batch 600/946 | Avg Loss: 2.4851
Finetune Epoch 3 | Batch 650/946 | Avg Loss: 2.4879
Finetune Epoch 3 | Batch 700/946 | Avg Loss: 2.4877
Finetune Epoch 3 | Batch 750/946 | Avg Loss: 2.4899
Finetune Epoch 3 | Batch 800/946 | Avg Loss: 2.4898
Finetune Epoch 3 | Batch 850/946 | Avg Loss: 2.4862
Finetune Epoch 3 | Batch 900/946 | Avg Loss: 2.4885
--- End of Finetune Epoch 3 | Train Loss: 2.4900 | Val Loss: 2.8842 | Val Acc: 0.0673 | Val F1: 0.2282 | LR: 0.000050 ---
Finetune Epoch 4 | Batch 50/946 | Avg Loss: 2.2346
Finetune Epoch 4 | Batch 100/946 | Avg Loss: 2.2306
Finetune Epoch 4 | Batch 150/946 | Avg Loss: 2.2600
Finetune Epoch 4 | Batch 200/946 | Avg Loss: 2.2758
Finetune Epoch 4 | Batch 250/946 | Avg Loss: 2.2875
Finetune Epoch 4 | Batch 300/946 | Avg Loss: 2.2926
Finetune Epoch 4 | Batch 350/946 | Avg Loss: 2.2865
Finetune Epoch 4 | Batch 400/946 | Avg Loss: 2.2840
Finetune Epoch 4 | Batch 450/946 | Avg Loss: 2.2957
Finetune Epoch 4 | Batch 500/946 | Avg Loss: 2.2940
Finetune Epoch 4 | Batch 550/946 | Avg Loss: 2.2940
Finetune Epoch 4 | Batch 600/946 | Avg Loss: 2.2953
Finetune Epoch 4 | Batch 650/946 | Avg Loss: 2.2928
Finetune Epoch 4 | Batch 700/946 | Avg Loss: 2.3012
Finetune Epoch 4 | Batch 750/946 | Avg Loss: 2.3023
Finetune Epoch 4 | Batch 800/946 | Avg Loss: 2.3034
Finetune Epoch 4 | Batch 850/946 | Avg Loss: 2.3049
Finetune Epoch 4 | Batch 900/946 | Avg Loss: 2.3083
--- End of Finetune Epoch 4 | Train Loss: 2.3087 | Val Loss: 2.9401 | Val Acc: 0.0988 | Val F1: 0.2541 | LR: 0.000050 ---
Finetune Epoch 5 | Batch 50/946 | Avg Loss: 2.0855
Finetune Epoch 5 | Batch 100/946 | Avg Loss: 2.1409
Finetune Epoch 5 | Batch 150/946 | Avg Loss: 2.1546
Finetune Epoch 5 | Batch 200/946 | Avg Loss: 2.1634
Finetune Epoch 5 | Batch 250/946 | Avg Loss: 2.1605
Finetune Epoch 5 | Batch 300/946 | Avg Loss: 2.1660
Finetune Epoch 5 | Batch 350/946 | Avg Loss: 2.1734
Finetune Epoch 5 | Batch 400/946 | Avg Loss: 2.1699
Finetune Epoch 5 | Batch 450/946 | Avg Loss: 2.1672
Finetune Epoch 5 | Batch 500/946 | Avg Loss: 2.1645
Finetune Epoch 5 | Batch 550/946 | Avg Loss: 2.1631
Finetune Epoch 5 | Batch 600/946 | Avg Loss: 2.1686
Finetune Epoch 5 | Batch 650/946 | Avg Loss: 2.1711
Finetune Epoch 5 | Batch 700/946 | Avg Loss: 2.1686
Finetune Epoch 5 | Batch 750/946 | Avg Loss: 2.1693
Finetune Epoch 5 | Batch 800/946 | Avg Loss: 2.1649
Finetune Epoch 5 | Batch 850/946 | Avg Loss: 2.1611
Finetune Epoch 5 | Batch 900/946 | Avg Loss: 2.1591
--- End of Finetune Epoch 5 | Train Loss: 2.1583 | Val Loss: 3.0505 | Val Acc: 0.1035 | Val F1: 0.2714 | LR: 0.000025 ---
Finetune Epoch 6 | Batch 50/946 | Avg Loss: 2.0553
Finetune Epoch 6 | Batch 100/946 | Avg Loss: 2.0259
Finetune Epoch 6 | Batch 150/946 | Avg Loss: 2.0326
Finetune Epoch 6 | Batch 200/946 | Avg Loss: 2.0192
Finetune Epoch 6 | Batch 250/946 | Avg Loss: 2.0199
Finetune Epoch 6 | Batch 300/946 | Avg Loss: 2.0111
Finetune Epoch 6 | Batch 350/946 | Avg Loss: 2.0037
Finetune Epoch 6 | Batch 400/946 | Avg Loss: 2.0098
Finetune Epoch 6 | Batch 450/946 | Avg Loss: 2.0003
Finetune Epoch 6 | Batch 500/946 | Avg Loss: 2.0082
Finetune Epoch 6 | Batch 550/946 | Avg Loss: 2.0024
Finetune Epoch 6 | Batch 600/946 | Avg Loss: 1.9947
Finetune Epoch 6 | Batch 650/946 | Avg Loss: 2.0003
Finetune Epoch 6 | Batch 700/946 | Avg Loss: 1.9972
Finetune Epoch 6 | Batch 750/946 | Avg Loss: 1.9984
Finetune Epoch 6 | Batch 800/946 | Avg Loss: 2.0027
Finetune Epoch 6 | Batch 850/946 | Avg Loss: 2.0049
Finetune Epoch 6 | Batch 900/946 | Avg Loss: 2.0025
--- End of Finetune Epoch 6 | Train Loss: 2.0053 | Val Loss: 3.2185 | Val Acc: 0.1015 | Val F1: 0.2645 | LR: 0.000025 ---

Fine-tuning finished.
Best fine-tuned QA model state_dict saved to models/TinyQA_20250904-191429/toy_llm_qasrl_finetuned.pth
Loading best model from models/TinyQA_20250904-191429/toy_llm_qasrl_finetuned.pth for final stats.
Final Training Set Accuracy: 0.1021 | Final F1: 0.2662
Final training statistics saved to 'models/TinyQA_20250904-191429/finetune_stats.json'

--- Running Validation Metrics on QA-SRL Validation Set for 'TinyQA' model ---
Loaded BEST finetuned model from models/TinyQA_20250904-191429/toy_llm_qasrl_finetuned.pth for validation.
Final Validation Accuracy: 0.0727 | Final Validation F1: 0.2149
Validation statistics saved to 'models/TinyQA_20250904-191429/validation_stats.json'
