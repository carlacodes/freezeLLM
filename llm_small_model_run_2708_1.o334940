Wed Aug 27 12:25:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:58:00.0 Off |                    0 |
| N/A   35C    P0             35W /  250W |       1MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
--- Loaded configuration 'small' from '/home/zceccgr/Scratch/freezeLLM/llm_scale_up/config.json' ---
Using device: cuda
Loading standard tokenizer ('bert-base-uncased')...
Standard vocabulary size: 30522
PAD token ID: 0
Instantiated Base Model: SmallQA with 11,038,720 trainable parameters.
Best pre-trained model will be saved to: models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth

--- Starting CLM Pre-training on nq_open for 'SmallQA' model ---
--- End of Pre-train Epoch 1 | Train Loss: 5.8812 | Validation Loss: 5.5624 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 2 | Train Loss: 5.2062 | Validation Loss: 5.3166 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 3 | Train Loss: 4.9378 | Validation Loss: 5.1850 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 4 | Train Loss: 4.7711 | Validation Loss: 5.1242 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 5 | Train Loss: 4.6571 | Validation Loss: 5.0798 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 6 | Train Loss: 4.5758 | Validation Loss: 5.0572 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 7 | Train Loss: 4.5136 | Validation Loss: 5.0378 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 8 | Train Loss: 4.4643 | Validation Loss: 5.0307 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 9 | Train Loss: 4.4260 | Validation Loss: 4.9971 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 10 | Train Loss: 4.3922 | Validation Loss: 5.0067 | LR: 0.000299 ---
--- End of Pre-train Epoch 11 | Train Loss: 4.3653 | Validation Loss: 4.9932 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 12 | Train Loss: 4.3400 | Validation Loss: 4.9857 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 13 | Train Loss: 4.3195 | Validation Loss: 4.9780 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 14 | Train Loss: 4.3021 | Validation Loss: 4.9911 | LR: 0.000299 ---
--- End of Pre-train Epoch 15 | Train Loss: 4.2863 | Validation Loss: 4.9764 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 16 | Train Loss: 4.2725 | Validation Loss: 4.9749 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 17 | Train Loss: 4.2596 | Validation Loss: 4.9670 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 18 | Train Loss: 4.2472 | Validation Loss: 4.9528 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 19 | Train Loss: 4.2365 | Validation Loss: 4.9714 | LR: 0.000299 ---
--- End of Pre-train Epoch 20 | Train Loss: 4.2280 | Validation Loss: 4.9635 | LR: 0.000299 ---
--- End of Pre-train Epoch 21 | Train Loss: 4.2173 | Validation Loss: 4.9590 | LR: 0.000299 ---
--- End of Pre-train Epoch 22 | Train Loss: 3.9397 | Validation Loss: 4.8836 | LR: 0.000150 ---
Validation loss improved. Saving model to models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 23 | Train Loss: 3.8558 | Validation Loss: 4.8978 | LR: 0.000150 ---
--- End of Pre-train Epoch 24 | Train Loss: 3.8356 | Validation Loss: 4.9125 | LR: 0.000150 ---
--- End of Pre-train Epoch 25 | Train Loss: 3.8283 | Validation Loss: 4.9171 | LR: 0.000150 ---
--- End of Pre-train Epoch 26 | Train Loss: 3.6361 | Validation Loss: 4.8878 | LR: 0.000075 ---
--- End of Pre-train Epoch 27 | Train Loss: 3.5839 | Validation Loss: 4.9088 | LR: 0.000075 ---

Early stopping triggered after 5 epochs without improvement.

Loading best pretrained model from models/SmallQA_20250827-122534/toy_llm_unified_pretrained.pth

Testing prompt completion after pretraining:
Prompt: 'The capital of France is'
Model completion: the capital of france is named for the fully crisis spain

--- Starting Fine-tuning on QA-SRL for 'SmallQA' model ---
Successfully loaded pre-trained weights into the QA model.
Loading and processing qa_srl dataset for 'train' split...
Finished processing 'train'. Found 7562 valid QA examples.
Starting fine-tuning for 6 epochs...
Finetune Epoch 1 | Batch 50/1891 | Avg Loss: 4.6667
Finetune Epoch 1 | Batch 100/1891 | Avg Loss: 4.2448
Finetune Epoch 1 | Batch 150/1891 | Avg Loss: 4.0351
Finetune Epoch 1 | Batch 200/1891 | Avg Loss: 3.9102
Finetune Epoch 1 | Batch 250/1891 | Avg Loss: 3.7930
Finetune Epoch 1 | Batch 300/1891 | Avg Loss: 3.7217
Finetune Epoch 1 | Batch 350/1891 | Avg Loss: 3.6389
Finetune Epoch 1 | Batch 400/1891 | Avg Loss: 3.5822
Finetune Epoch 1 | Batch 450/1891 | Avg Loss: 3.5346
Finetune Epoch 1 | Batch 500/1891 | Avg Loss: 3.4984
Finetune Epoch 1 | Batch 550/1891 | Avg Loss: 3.4614
Finetune Epoch 1 | Batch 600/1891 | Avg Loss: 3.4293
Finetune Epoch 1 | Batch 650/1891 | Avg Loss: 3.3913
Finetune Epoch 1 | Batch 700/1891 | Avg Loss: 3.3682
Finetune Epoch 1 | Batch 750/1891 | Avg Loss: 3.3474
Finetune Epoch 1 | Batch 800/1891 | Avg Loss: 3.3247
Finetune Epoch 1 | Batch 850/1891 | Avg Loss: 3.3102
Finetune Epoch 1 | Batch 900/1891 | Avg Loss: 3.2956
Finetune Epoch 1 | Batch 950/1891 | Avg Loss: 3.2787
Finetune Epoch 1 | Batch 1000/1891 | Avg Loss: 3.2562
Finetune Epoch 1 | Batch 1050/1891 | Avg Loss: 3.2421
Finetune Epoch 1 | Batch 1100/1891 | Avg Loss: 3.2362
Finetune Epoch 1 | Batch 1150/1891 | Avg Loss: 3.2255
Finetune Epoch 1 | Batch 1200/1891 | Avg Loss: 3.2083
Finetune Epoch 1 | Batch 1250/1891 | Avg Loss: 3.1982
Finetune Epoch 1 | Batch 1300/1891 | Avg Loss: 3.1880
Finetune Epoch 1 | Batch 1350/1891 | Avg Loss: 3.1771
Finetune Epoch 1 | Batch 1400/1891 | Avg Loss: 3.1649
Finetune Epoch 1 | Batch 1450/1891 | Avg Loss: 3.1569
Finetune Epoch 1 | Batch 1500/1891 | Avg Loss: 3.1460
Finetune Epoch 1 | Batch 1550/1891 | Avg Loss: 3.1351
Finetune Epoch 1 | Batch 1600/1891 | Avg Loss: 3.1244
Finetune Epoch 1 | Batch 1650/1891 | Avg Loss: 3.1121
Finetune Epoch 1 | Batch 1700/1891 | Avg Loss: 3.1069
Finetune Epoch 1 | Batch 1750/1891 | Avg Loss: 3.0997
Finetune Epoch 1 | Batch 1800/1891 | Avg Loss: 3.0923
Finetune Epoch 1 | Batch 1850/1891 | Avg Loss: 3.0874
--- End of Finetune Epoch 1 | Average QA Loss: 3.0825 | LR: 0.000050 ---
Finetune Epoch 2 | Batch 50/1891 | Avg Loss: 2.6475
Finetune Epoch 2 | Batch 100/1891 | Avg Loss: 2.6227
Finetune Epoch 2 | Batch 150/1891 | Avg Loss: 2.6462
Finetune Epoch 2 | Batch 200/1891 | Avg Loss: 2.6387
Finetune Epoch 2 | Batch 250/1891 | Avg Loss: 2.6314
Finetune Epoch 2 | Batch 300/1891 | Avg Loss: 2.6256
Finetune Epoch 2 | Batch 350/1891 | Avg Loss: 2.6168
Finetune Epoch 2 | Batch 400/1891 | Avg Loss: 2.6073
Finetune Epoch 2 | Batch 450/1891 | Avg Loss: 2.5899
Finetune Epoch 2 | Batch 500/1891 | Avg Loss: 2.5831
Finetune Epoch 2 | Batch 550/1891 | Avg Loss: 2.5830
Finetune Epoch 2 | Batch 600/1891 | Avg Loss: 2.5769
Finetune Epoch 2 | Batch 650/1891 | Avg Loss: 2.5708
Finetune Epoch 2 | Batch 700/1891 | Avg Loss: 2.5650
Finetune Epoch 2 | Batch 750/1891 | Avg Loss: 2.5611
Finetune Epoch 2 | Batch 800/1891 | Avg Loss: 2.5567
Finetune Epoch 2 | Batch 850/1891 | Avg Loss: 2.5568
Finetune Epoch 2 | Batch 900/1891 | Avg Loss: 2.5534
Finetune Epoch 2 | Batch 950/1891 | Avg Loss: 2.5508
Finetune Epoch 2 | Batch 1000/1891 | Avg Loss: 2.5474
Finetune Epoch 2 | Batch 1050/1891 | Avg Loss: 2.5362
Finetune Epoch 2 | Batch 1100/1891 | Avg Loss: 2.5349
Finetune Epoch 2 | Batch 1150/1891 | Avg Loss: 2.5326
Finetune Epoch 2 | Batch 1200/1891 | Avg Loss: 2.5356
Finetune Epoch 2 | Batch 1250/1891 | Avg Loss: 2.5326
Finetune Epoch 2 | Batch 1300/1891 | Avg Loss: 2.5295
Finetune Epoch 2 | Batch 1350/1891 | Avg Loss: 2.5263
Finetune Epoch 2 | Batch 1400/1891 | Avg Loss: 2.5270
Finetune Epoch 2 | Batch 1450/1891 | Avg Loss: 2.5216
Finetune Epoch 2 | Batch 1500/1891 | Avg Loss: 2.5170
Finetune Epoch 2 | Batch 1550/1891 | Avg Loss: 2.5171
Finetune Epoch 2 | Batch 1600/1891 | Avg Loss: 2.5095
Finetune Epoch 2 | Batch 1650/1891 | Avg Loss: 2.5067
Finetune Epoch 2 | Batch 1700/1891 | Avg Loss: 2.5061
Finetune Epoch 2 | Batch 1750/1891 | Avg Loss: 2.5044
Finetune Epoch 2 | Batch 1800/1891 | Avg Loss: 2.5005
Finetune Epoch 2 | Batch 1850/1891 | Avg Loss: 2.5003
--- End of Finetune Epoch 2 | Average QA Loss: 2.5000 | LR: 0.000050 ---
Finetune Epoch 3 | Batch 50/1891 | Avg Loss: 2.3142
Finetune Epoch 3 | Batch 100/1891 | Avg Loss: 2.2665
Finetune Epoch 3 | Batch 150/1891 | Avg Loss: 2.1697
Finetune Epoch 3 | Batch 200/1891 | Avg Loss: 2.1583
Finetune Epoch 3 | Batch 250/1891 | Avg Loss: 2.1311
Finetune Epoch 3 | Batch 300/1891 | Avg Loss: 2.1184
Finetune Epoch 3 | Batch 350/1891 | Avg Loss: 2.1115
Finetune Epoch 3 | Batch 400/1891 | Avg Loss: 2.1013
Finetune Epoch 3 | Batch 450/1891 | Avg Loss: 2.1032
Finetune Epoch 3 | Batch 500/1891 | Avg Loss: 2.1103
Finetune Epoch 3 | Batch 550/1891 | Avg Loss: 2.1148
Finetune Epoch 3 | Batch 600/1891 | Avg Loss: 2.1137
Finetune Epoch 3 | Batch 650/1891 | Avg Loss: 2.1056
Finetune Epoch 3 | Batch 700/1891 | Avg Loss: 2.0994
Finetune Epoch 3 | Batch 750/1891 | Avg Loss: 2.0966
Finetune Epoch 3 | Batch 800/1891 | Avg Loss: 2.0967
Finetune Epoch 3 | Batch 850/1891 | Avg Loss: 2.0928
Finetune Epoch 3 | Batch 900/1891 | Avg Loss: 2.0917
Finetune Epoch 3 | Batch 950/1891 | Avg Loss: 2.0893
Finetune Epoch 3 | Batch 1000/1891 | Avg Loss: 2.0919
Finetune Epoch 3 | Batch 1050/1891 | Avg Loss: 2.0860
Finetune Epoch 3 | Batch 1100/1891 | Avg Loss: 2.0871
Finetune Epoch 3 | Batch 1150/1891 | Avg Loss: 2.0832
Finetune Epoch 3 | Batch 1200/1891 | Avg Loss: 2.0893
Finetune Epoch 3 | Batch 1250/1891 | Avg Loss: 2.0881
Finetune Epoch 3 | Batch 1300/1891 | Avg Loss: 2.0889
Finetune Epoch 3 | Batch 1350/1891 | Avg Loss: 2.0920
Finetune Epoch 3 | Batch 1400/1891 | Avg Loss: 2.0929
Finetune Epoch 3 | Batch 1450/1891 | Avg Loss: 2.0947
Finetune Epoch 3 | Batch 1500/1891 | Avg Loss: 2.0926
Finetune Epoch 3 | Batch 1550/1891 | Avg Loss: 2.0924
Finetune Epoch 3 | Batch 1600/1891 | Avg Loss: 2.0885
Finetune Epoch 3 | Batch 1650/1891 | Avg Loss: 2.0866
Finetune Epoch 3 | Batch 1700/1891 | Avg Loss: 2.0881
Finetune Epoch 3 | Batch 1750/1891 | Avg Loss: 2.0882
Finetune Epoch 3 | Batch 1800/1891 | Avg Loss: 2.0887
Finetune Epoch 3 | Batch 1850/1891 | Avg Loss: 2.0905
--- End of Finetune Epoch 3 | Average QA Loss: 2.0950 | LR: 0.000050 ---
Finetune Epoch 4 | Batch 50/1891 | Avg Loss: 1.8489
Finetune Epoch 4 | Batch 100/1891 | Avg Loss: 1.7444
Finetune Epoch 4 | Batch 150/1891 | Avg Loss: 1.7696
Finetune Epoch 4 | Batch 200/1891 | Avg Loss: 1.7776
Finetune Epoch 4 | Batch 250/1891 | Avg Loss: 1.7809
Finetune Epoch 4 | Batch 300/1891 | Avg Loss: 1.7748
Finetune Epoch 4 | Batch 350/1891 | Avg Loss: 1.7790
Finetune Epoch 4 | Batch 400/1891 | Avg Loss: 1.7839
Finetune Epoch 4 | Batch 450/1891 | Avg Loss: 1.7828
Finetune Epoch 4 | Batch 500/1891 | Avg Loss: 1.7789
Finetune Epoch 4 | Batch 550/1891 | Avg Loss: 1.7803
Finetune Epoch 4 | Batch 600/1891 | Avg Loss: 1.7779
Finetune Epoch 4 | Batch 650/1891 | Avg Loss: 1.7765
Finetune Epoch 4 | Batch 700/1891 | Avg Loss: 1.7812
Finetune Epoch 4 | Batch 750/1891 | Avg Loss: 1.7863
Finetune Epoch 4 | Batch 800/1891 | Avg Loss: 1.7962
Finetune Epoch 4 | Batch 850/1891 | Avg Loss: 1.7975
Finetune Epoch 4 | Batch 900/1891 | Avg Loss: 1.7946
Finetune Epoch 4 | Batch 950/1891 | Avg Loss: 1.7973
Finetune Epoch 4 | Batch 1000/1891 | Avg Loss: 1.8030
Finetune Epoch 4 | Batch 1050/1891 | Avg Loss: 1.8022
Finetune Epoch 4 | Batch 1100/1891 | Avg Loss: 1.7972
Finetune Epoch 4 | Batch 1150/1891 | Avg Loss: 1.8013
Finetune Epoch 4 | Batch 1200/1891 | Avg Loss: 1.8014
Finetune Epoch 4 | Batch 1250/1891 | Avg Loss: 1.7996
Finetune Epoch 4 | Batch 1300/1891 | Avg Loss: 1.8018
Finetune Epoch 4 | Batch 1350/1891 | Avg Loss: 1.8022
Finetune Epoch 4 | Batch 1400/1891 | Avg Loss: 1.8076
Finetune Epoch 4 | Batch 1450/1891 | Avg Loss: 1.8119
Finetune Epoch 4 | Batch 1500/1891 | Avg Loss: 1.8094
Finetune Epoch 4 | Batch 1550/1891 | Avg Loss: 1.8130
Finetune Epoch 4 | Batch 1600/1891 | Avg Loss: 1.8121
Finetune Epoch 4 | Batch 1650/1891 | Avg Loss: 1.8146
Finetune Epoch 4 | Batch 1700/1891 | Avg Loss: 1.8139
Finetune Epoch 4 | Batch 1750/1891 | Avg Loss: 1.8123
Finetune Epoch 4 | Batch 1800/1891 | Avg Loss: 1.8154
Finetune Epoch 4 | Batch 1850/1891 | Avg Loss: 1.8154
--- End of Finetune Epoch 4 | Average QA Loss: 1.8167 | LR: 0.000050 ---
Finetune Epoch 5 | Batch 50/1891 | Avg Loss: 1.4321
Finetune Epoch 5 | Batch 100/1891 | Avg Loss: 1.4607
Finetune Epoch 5 | Batch 150/1891 | Avg Loss: 1.4478
Finetune Epoch 5 | Batch 200/1891 | Avg Loss: 1.4902
Finetune Epoch 5 | Batch 250/1891 | Avg Loss: 1.5060
Finetune Epoch 5 | Batch 300/1891 | Avg Loss: 1.5152
Finetune Epoch 5 | Batch 350/1891 | Avg Loss: 1.5118
Finetune Epoch 5 | Batch 400/1891 | Avg Loss: 1.5017
Finetune Epoch 5 | Batch 450/1891 | Avg Loss: 1.5071
Finetune Epoch 5 | Batch 500/1891 | Avg Loss: 1.5065
Finetune Epoch 5 | Batch 550/1891 | Avg Loss: 1.5036
Finetune Epoch 5 | Batch 600/1891 | Avg Loss: 1.5034
Finetune Epoch 5 | Batch 650/1891 | Avg Loss: 1.5088
Finetune Epoch 5 | Batch 700/1891 | Avg Loss: 1.5080
Finetune Epoch 5 | Batch 750/1891 | Avg Loss: 1.5111
Finetune Epoch 5 | Batch 800/1891 | Avg Loss: 1.5173
Finetune Epoch 5 | Batch 850/1891 | Avg Loss: 1.5190
Finetune Epoch 5 | Batch 900/1891 | Avg Loss: 1.5218
Finetune Epoch 5 | Batch 950/1891 | Avg Loss: 1.5269
Finetune Epoch 5 | Batch 1000/1891 | Avg Loss: 1.5327
Finetune Epoch 5 | Batch 1050/1891 | Avg Loss: 1.5355
Finetune Epoch 5 | Batch 1100/1891 | Avg Loss: 1.5403
Finetune Epoch 5 | Batch 1150/1891 | Avg Loss: 1.5428
Finetune Epoch 5 | Batch 1200/1891 | Avg Loss: 1.5428
Finetune Epoch 5 | Batch 1250/1891 | Avg Loss: 1.5484
Finetune Epoch 5 | Batch 1300/1891 | Avg Loss: 1.5529
Finetune Epoch 5 | Batch 1350/1891 | Avg Loss: 1.5580
Finetune Epoch 5 | Batch 1400/1891 | Avg Loss: 1.5684
Finetune Epoch 5 | Batch 1450/1891 | Avg Loss: 1.5706
Finetune Epoch 5 | Batch 1500/1891 | Avg Loss: 1.5714
Finetune Epoch 5 | Batch 1550/1891 | Avg Loss: 1.5744
Finetune Epoch 5 | Batch 1600/1891 | Avg Loss: 1.5777
Finetune Epoch 5 | Batch 1650/1891 | Avg Loss: 1.5796
Finetune Epoch 5 | Batch 1700/1891 | Avg Loss: 1.5794
Finetune Epoch 5 | Batch 1750/1891 | Avg Loss: 1.5768
Finetune Epoch 5 | Batch 1800/1891 | Avg Loss: 1.5769
Finetune Epoch 5 | Batch 1850/1891 | Avg Loss: 1.5802
--- End of Finetune Epoch 5 | Average QA Loss: 1.5847 | LR: 0.000050 ---
Finetune Epoch 6 | Batch 50/1891 | Avg Loss: 1.3485
Finetune Epoch 6 | Batch 100/1891 | Avg Loss: 1.3570
Finetune Epoch 6 | Batch 150/1891 | Avg Loss: 1.3436
Finetune Epoch 6 | Batch 200/1891 | Avg Loss: 1.3284
Finetune Epoch 6 | Batch 250/1891 | Avg Loss: 1.3270
Finetune Epoch 6 | Batch 300/1891 | Avg Loss: 1.3242
Finetune Epoch 6 | Batch 350/1891 | Avg Loss: 1.3280
Finetune Epoch 6 | Batch 400/1891 | Avg Loss: 1.3288
Finetune Epoch 6 | Batch 450/1891 | Avg Loss: 1.3329
Finetune Epoch 6 | Batch 500/1891 | Avg Loss: 1.3285
Finetune Epoch 6 | Batch 550/1891 | Avg Loss: 1.3319
Finetune Epoch 6 | Batch 600/1891 | Avg Loss: 1.3405
Finetune Epoch 6 | Batch 650/1891 | Avg Loss: 1.3456
Finetune Epoch 6 | Batch 700/1891 | Avg Loss: 1.3519
Finetune Epoch 6 | Batch 750/1891 | Avg Loss: 1.3472
Finetune Epoch 6 | Batch 800/1891 | Avg Loss: 1.3521
Finetune Epoch 6 | Batch 850/1891 | Avg Loss: 1.3515
Finetune Epoch 6 | Batch 900/1891 | Avg Loss: 1.3533
Finetune Epoch 6 | Batch 950/1891 | Avg Loss: 1.3543
Finetune Epoch 6 | Batch 1000/1891 | Avg Loss: 1.3612
Finetune Epoch 6 | Batch 1050/1891 | Avg Loss: 1.3634
Finetune Epoch 6 | Batch 1100/1891 | Avg Loss: 1.3670
Finetune Epoch 6 | Batch 1150/1891 | Avg Loss: 1.3670
Finetune Epoch 6 | Batch 1200/1891 | Avg Loss: 1.3673
Finetune Epoch 6 | Batch 1250/1891 | Avg Loss: 1.3704
Finetune Epoch 6 | Batch 1300/1891 | Avg Loss: 1.3695
Finetune Epoch 6 | Batch 1350/1891 | Avg Loss: 1.3770
Finetune Epoch 6 | Batch 1400/1891 | Avg Loss: 1.3792
Finetune Epoch 6 | Batch 1450/1891 | Avg Loss: 1.3796
Finetune Epoch 6 | Batch 1500/1891 | Avg Loss: 1.3839
Finetune Epoch 6 | Batch 1550/1891 | Avg Loss: 1.3875
Finetune Epoch 6 | Batch 1600/1891 | Avg Loss: 1.3913
Finetune Epoch 6 | Batch 1650/1891 | Avg Loss: 1.3956
Finetune Epoch 6 | Batch 1700/1891 | Avg Loss: 1.3948
Finetune Epoch 6 | Batch 1750/1891 | Avg Loss: 1.4007
Finetune Epoch 6 | Batch 1800/1891 | Avg Loss: 1.4040
Finetune Epoch 6 | Batch 1850/1891 | Avg Loss: 1.4036
--- End of Finetune Epoch 6 | Average QA Loss: 1.4057 | LR: 0.000050 ---

Fine-tuning finished.
Fine-tuned QA model state_dict saved to models/SmallQA_20250827-122534/toy_llm_qasrl_finetuned.pth
Final Training Set Accuracy: 0.4944 | Final F1: 0.6238
Final training statistics saved to 'models/SmallQA_20250827-122534/finetune_stats.json'

--- Running Validation Metrics on QA-SRL Validation Set for 'SmallQA' model ---
Loaded finetuned model from models/SmallQA_20250827-122534/toy_llm_qasrl_finetuned.pth for validation.
Loading and processing qa_srl dataset for 'validation' split...
Finished processing 'validation'. Found 2571 valid QA examples.
Final Validation Accuracy: 0.1000 | Final Validation F1: 0.2748
Validation statistics saved to 'models/SmallQA_20250827-122534/validation_stats.json'
