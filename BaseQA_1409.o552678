Mon Sep 15 11:26:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:2F:00.0 Off |                    0 |
| N/A   37C    P0             35W /  250W |       1MiB /  40960MiB |      5%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Starting Python script for BaseQA model...
--- Loaded configuration 'base' from '/home/zceccgr/Scratch/freezeLLM/llm_scale_up/config.json' ---
Using device: cuda
Loading standard tokenizer ('bert-base-uncased')...
Standard vocabulary size: 30522
PAD token ID: 0
Instantiated Base Model: BaseQA with 34,804,736 trainable parameters.
Best pre-trained model will be saved to: models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth

--- Starting CLM Pre-training on nq_open for 'BaseQA' model ---
--- End of Pre-train Epoch 1 | Train Loss: 5.4418 | Validation Loss: 5.1359 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 2 | Train Loss: 4.9036 | Validation Loss: 4.9079 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 3 | Train Loss: 4.7201 | Validation Loss: 4.7749 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 4 | Train Loss: 4.5931 | Validation Loss: 4.6847 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 5 | Train Loss: 4.4941 | Validation Loss: 4.6153 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 6 | Train Loss: 4.4111 | Validation Loss: 4.5537 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 7 | Train Loss: 4.3387 | Validation Loss: 4.5058 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 8 | Train Loss: 4.2757 | Validation Loss: 4.4624 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 9 | Train Loss: 4.2180 | Validation Loss: 4.4266 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 10 | Train Loss: 4.1655 | Validation Loss: 4.3973 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 11 | Train Loss: 4.1175 | Validation Loss: 4.3669 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 12 | Train Loss: 4.0721 | Validation Loss: 4.3448 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 13 | Train Loss: 4.0303 | Validation Loss: 4.3232 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 14 | Train Loss: 3.9915 | Validation Loss: 4.3035 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 15 | Train Loss: 3.9531 | Validation Loss: 4.2862 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 16 | Train Loss: 3.9178 | Validation Loss: 4.2664 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 17 | Train Loss: 3.8836 | Validation Loss: 4.2564 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 18 | Train Loss: 3.8518 | Validation Loss: 4.2419 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 19 | Train Loss: 3.8203 | Validation Loss: 4.2308 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 20 | Train Loss: 3.7914 | Validation Loss: 4.2237 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 21 | Train Loss: 3.7628 | Validation Loss: 4.2181 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 22 | Train Loss: 3.7349 | Validation Loss: 4.2114 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 23 | Train Loss: 3.7097 | Validation Loss: 4.2009 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 24 | Train Loss: 3.6845 | Validation Loss: 4.1983 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 25 | Train Loss: 3.6606 | Validation Loss: 4.1941 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 26 | Train Loss: 3.6369 | Validation Loss: 4.1922 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 27 | Train Loss: 3.6145 | Validation Loss: 4.1837 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 28 | Train Loss: 3.5923 | Validation Loss: 4.1848 | LR: 0.000010 ---
--- End of Pre-train Epoch 29 | Train Loss: 3.5722 | Validation Loss: 4.1809 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 30 | Train Loss: 3.5520 | Validation Loss: 4.1786 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 31 | Train Loss: 3.5323 | Validation Loss: 4.1802 | LR: 0.000010 ---
--- End of Pre-train Epoch 32 | Train Loss: 3.5136 | Validation Loss: 4.1828 | LR: 0.000010 ---
--- End of Pre-train Epoch 33 | Train Loss: 3.4960 | Validation Loss: 4.1772 | LR: 0.000010 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 34 | Train Loss: 3.4775 | Validation Loss: 4.1829 | LR: 0.000010 ---
--- End of Pre-train Epoch 35 | Train Loss: 3.4607 | Validation Loss: 4.1812 | LR: 0.000010 ---
--- End of Pre-train Epoch 36 | Train Loss: 3.4438 | Validation Loss: 4.1787 | LR: 0.000010 ---
--- End of Pre-train Epoch 37 | Train Loss: 3.3824 | Validation Loss: 4.1636 | LR: 0.000005 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 38 | Train Loss: 3.3639 | Validation Loss: 4.1623 | LR: 0.000005 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 39 | Train Loss: 3.3511 | Validation Loss: 4.1608 | LR: 0.000005 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 40 | Train Loss: 3.3392 | Validation Loss: 4.1645 | LR: 0.000005 ---
--- End of Pre-train Epoch 41 | Train Loss: 3.3284 | Validation Loss: 4.1662 | LR: 0.000005 ---
--- End of Pre-train Epoch 42 | Train Loss: 3.3185 | Validation Loss: 4.1674 | LR: 0.000005 ---
--- End of Pre-train Epoch 43 | Train Loss: 3.2851 | Validation Loss: 4.1568 | LR: 0.000002 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 44 | Train Loss: 3.2754 | Validation Loss: 4.1606 | LR: 0.000002 ---
--- End of Pre-train Epoch 45 | Train Loss: 3.2681 | Validation Loss: 4.1593 | LR: 0.000002 ---
--- End of Pre-train Epoch 46 | Train Loss: 3.2614 | Validation Loss: 4.1604 | LR: 0.000002 ---
--- End of Pre-train Epoch 47 | Train Loss: 3.2451 | Validation Loss: 4.1545 | LR: 0.000001 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 48 | Train Loss: 3.2399 | Validation Loss: 4.1531 | LR: 0.000001 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 49 | Train Loss: 3.2358 | Validation Loss: 4.1542 | LR: 0.000001 ---
--- End of Pre-train Epoch 50 | Train Loss: 3.2320 | Validation Loss: 4.1549 | LR: 0.000001 ---
--- End of Pre-train Epoch 51 | Train Loss: 3.2281 | Validation Loss: 4.1558 | LR: 0.000001 ---
--- End of Pre-train Epoch 52 | Train Loss: 3.2216 | Validation Loss: 4.1522 | LR: 0.000001 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 53 | Train Loss: 3.2195 | Validation Loss: 4.1512 | LR: 0.000001 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 54 | Train Loss: 3.2174 | Validation Loss: 4.1521 | LR: 0.000001 ---
--- End of Pre-train Epoch 55 | Train Loss: 3.2163 | Validation Loss: 4.1513 | LR: 0.000001 ---
--- End of Pre-train Epoch 56 | Train Loss: 3.2145 | Validation Loss: 4.1500 | LR: 0.000001 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 57 | Train Loss: 3.2124 | Validation Loss: 4.1509 | LR: 0.000001 ---
--- End of Pre-train Epoch 58 | Train Loss: 3.2115 | Validation Loss: 4.1525 | LR: 0.000001 ---
--- End of Pre-train Epoch 59 | Train Loss: 3.2104 | Validation Loss: 4.1506 | LR: 0.000001 ---
--- End of Pre-train Epoch 60 | Train Loss: 3.2077 | Validation Loss: 4.1492 | LR: 0.000000 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 61 | Train Loss: 3.2072 | Validation Loss: 4.1477 | LR: 0.000000 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 62 | Train Loss: 3.2073 | Validation Loss: 4.1478 | LR: 0.000000 ---
--- End of Pre-train Epoch 63 | Train Loss: 3.2076 | Validation Loss: 4.1475 | LR: 0.000000 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 64 | Train Loss: 3.2073 | Validation Loss: 4.1475 | LR: 0.000000 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 65 | Train Loss: 3.2071 | Validation Loss: 4.1459 | LR: 0.000000 ---
Validation loss improved. Saving model to models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 66 | Train Loss: 3.2051 | Validation Loss: 4.1468 | LR: 0.000000 ---
--- End of Pre-train Epoch 67 | Train Loss: 3.2049 | Validation Loss: 4.1467 | LR: 0.000000 ---
--- End of Pre-train Epoch 68 | Train Loss: 3.2028 | Validation Loss: 4.1474 | LR: 0.000000 ---
--- End of Pre-train Epoch 69 | Train Loss: 3.2020 | Validation Loss: 4.1477 | LR: 0.000000 ---
--- End of Pre-train Epoch 70 | Train Loss: 3.2021 | Validation Loss: 4.1480 | LR: 0.000000 ---

Early stopping triggered after 5 epochs without improvement.

Loading best pretrained model from models/BaseQA_20250915-112724/toy_llm_unified_pretrained.pth

Testing prompt completion after pretraining:
Prompt: 'Question: where was the statue of liberty originally built'
Model completion: question : where was the statue of liberty originally built by answer : damn te cleansing

--- Starting Fine-tuning on QA-SRL for 'BaseQA' model ---
Successfully loaded pre-trained weights into the QA model.
Loading and processing qa_srl dataset for 'train' split...
Finished processing 'train'. Found 5031 valid QA examples.
Loading and processing qa_srl dataset for 'validation' split...
Finished processing 'validation'. Found 1686 valid QA examples.
Starting fine-tuning for 6 epochs...
Finetune Epoch 1 | Batch 50/2516 | Avg Loss: 4.7742
Finetune Epoch 1 | Batch 100/2516 | Avg Loss: 4.3772
Finetune Epoch 1 | Batch 150/2516 | Avg Loss: 4.1988
Finetune Epoch 1 | Batch 200/2516 | Avg Loss: 4.0329
Finetune Epoch 1 | Batch 250/2516 | Avg Loss: 3.9441
Finetune Epoch 1 | Batch 300/2516 | Avg Loss: 3.8802
Finetune Epoch 1 | Batch 350/2516 | Avg Loss: 3.8506
Finetune Epoch 1 | Batch 400/2516 | Avg Loss: 3.8352
Finetune Epoch 1 | Batch 450/2516 | Avg Loss: 3.7990
Finetune Epoch 1 | Batch 500/2516 | Avg Loss: 3.7625
Finetune Epoch 1 | Batch 550/2516 | Avg Loss: 3.7499
Finetune Epoch 1 | Batch 600/2516 | Avg Loss: 3.7390
Finetune Epoch 1 | Batch 650/2516 | Avg Loss: 3.7236
Finetune Epoch 1 | Batch 700/2516 | Avg Loss: 3.7042
Finetune Epoch 1 | Batch 750/2516 | Avg Loss: 3.6948
Finetune Epoch 1 | Batch 800/2516 | Avg Loss: 3.6971
Finetune Epoch 1 | Batch 850/2516 | Avg Loss: 3.6889
Finetune Epoch 1 | Batch 900/2516 | Avg Loss: 3.6747
Finetune Epoch 1 | Batch 950/2516 | Avg Loss: 3.6715
Finetune Epoch 1 | Batch 1000/2516 | Avg Loss: 3.6627
Finetune Epoch 1 | Batch 1050/2516 | Avg Loss: 3.6536
Finetune Epoch 1 | Batch 1100/2516 | Avg Loss: 3.6451
Finetune Epoch 1 | Batch 1150/2516 | Avg Loss: 3.6368
Finetune Epoch 1 | Batch 1200/2516 | Avg Loss: 3.6303
Finetune Epoch 1 | Batch 1250/2516 | Avg Loss: 3.6233
Finetune Epoch 1 | Batch 1300/2516 | Avg Loss: 3.6177
Finetune Epoch 1 | Batch 1350/2516 | Avg Loss: 3.6142
Finetune Epoch 1 | Batch 1400/2516 | Avg Loss: 3.5979
Finetune Epoch 1 | Batch 1450/2516 | Avg Loss: 3.5994
Finetune Epoch 1 | Batch 1500/2516 | Avg Loss: 3.5860
Finetune Epoch 1 | Batch 1550/2516 | Avg Loss: 3.5841
Finetune Epoch 1 | Batch 1600/2516 | Avg Loss: 3.5757
Finetune Epoch 1 | Batch 1650/2516 | Avg Loss: 3.5660
Finetune Epoch 1 | Batch 1700/2516 | Avg Loss: 3.5588
Finetune Epoch 1 | Batch 1750/2516 | Avg Loss: 3.5497
Finetune Epoch 1 | Batch 1800/2516 | Avg Loss: 3.5476
Finetune Epoch 1 | Batch 1850/2516 | Avg Loss: 3.5448
Finetune Epoch 1 | Batch 1900/2516 | Avg Loss: 3.5379
Finetune Epoch 1 | Batch 1950/2516 | Avg Loss: 3.5286
Finetune Epoch 1 | Batch 2000/2516 | Avg Loss: 3.5206
Finetune Epoch 1 | Batch 2050/2516 | Avg Loss: 3.5124
Finetune Epoch 1 | Batch 2100/2516 | Avg Loss: 3.5097
Finetune Epoch 1 | Batch 2150/2516 | Avg Loss: 3.5048
Finetune Epoch 1 | Batch 2200/2516 | Avg Loss: 3.4996
Finetune Epoch 1 | Batch 2250/2516 | Avg Loss: 3.4957
Finetune Epoch 1 | Batch 2300/2516 | Avg Loss: 3.4882
Finetune Epoch 1 | Batch 2350/2516 | Avg Loss: 3.4835
Finetune Epoch 1 | Batch 2400/2516 | Avg Loss: 3.4776
Finetune Epoch 1 | Batch 2450/2516 | Avg Loss: 3.4713
Finetune Epoch 1 | Batch 2500/2516 | Avg Loss: 3.4696
--- End of Finetune Epoch 1 | Train Loss: 3.4687 | Val Loss: 3.2155 | Val Acc: 0.0480 | Val F1: 0.1900 | LR: 0.000030 ---
Validation loss improved. Saving best model to models/BaseQA_20250915-112724/toy_llm_qasrl_finetuned.pth
Finetune Epoch 2 | Batch 50/2516 | Avg Loss: 3.0637
Finetune Epoch 2 | Batch 100/2516 | Avg Loss: 2.9848
Finetune Epoch 2 | Batch 150/2516 | Avg Loss: 2.9895
Finetune Epoch 2 | Batch 200/2516 | Avg Loss: 2.9907
Finetune Epoch 2 | Batch 250/2516 | Avg Loss: 3.0000
Finetune Epoch 2 | Batch 300/2516 | Avg Loss: 2.9870
Finetune Epoch 2 | Batch 350/2516 | Avg Loss: 2.9577
Finetune Epoch 2 | Batch 400/2516 | Avg Loss: 2.9685
Finetune Epoch 2 | Batch 450/2516 | Avg Loss: 2.9731
Finetune Epoch 2 | Batch 500/2516 | Avg Loss: 2.9712
Finetune Epoch 2 | Batch 550/2516 | Avg Loss: 2.9734
Finetune Epoch 2 | Batch 600/2516 | Avg Loss: 2.9738
Finetune Epoch 2 | Batch 650/2516 | Avg Loss: 2.9935
Finetune Epoch 2 | Batch 700/2516 | Avg Loss: 2.9951
Finetune Epoch 2 | Batch 750/2516 | Avg Loss: 3.0011
Finetune Epoch 2 | Batch 800/2516 | Avg Loss: 3.0013
Finetune Epoch 2 | Batch 850/2516 | Avg Loss: 3.0032
Finetune Epoch 2 | Batch 900/2516 | Avg Loss: 3.0153
Finetune Epoch 2 | Batch 950/2516 | Avg Loss: 3.0072
Finetune Epoch 2 | Batch 1000/2516 | Avg Loss: 3.0122
Finetune Epoch 2 | Batch 1050/2516 | Avg Loss: 3.0064
Finetune Epoch 2 | Batch 1100/2516 | Avg Loss: 3.0130
Finetune Epoch 2 | Batch 1150/2516 | Avg Loss: 3.0117
Finetune Epoch 2 | Batch 1200/2516 | Avg Loss: 3.0079
Finetune Epoch 2 | Batch 1250/2516 | Avg Loss: 3.0074
Finetune Epoch 2 | Batch 1300/2516 | Avg Loss: 3.0086
Finetune Epoch 2 | Batch 1350/2516 | Avg Loss: 3.0106
Finetune Epoch 2 | Batch 1400/2516 | Avg Loss: 3.0093
Finetune Epoch 2 | Batch 1450/2516 | Avg Loss: 3.0081
Finetune Epoch 2 | Batch 1500/2516 | Avg Loss: 3.0109
Finetune Epoch 2 | Batch 1550/2516 | Avg Loss: 3.0087
Finetune Epoch 2 | Batch 1600/2516 | Avg Loss: 3.0094
Finetune Epoch 2 | Batch 1650/2516 | Avg Loss: 3.0066
Finetune Epoch 2 | Batch 1700/2516 | Avg Loss: 3.0045
Finetune Epoch 2 | Batch 1750/2516 | Avg Loss: 3.0055
Finetune Epoch 2 | Batch 1800/2516 | Avg Loss: 3.0041
Finetune Epoch 2 | Batch 1850/2516 | Avg Loss: 2.9980
Finetune Epoch 2 | Batch 1900/2516 | Avg Loss: 3.0003
Finetune Epoch 2 | Batch 1950/2516 | Avg Loss: 2.9979
Finetune Epoch 2 | Batch 2000/2516 | Avg Loss: 2.9975
Finetune Epoch 2 | Batch 2050/2516 | Avg Loss: 2.9919
Finetune Epoch 2 | Batch 2100/2516 | Avg Loss: 2.9938
Finetune Epoch 2 | Batch 2150/2516 | Avg Loss: 2.9914
Finetune Epoch 2 | Batch 2200/2516 | Avg Loss: 2.9945
Finetune Epoch 2 | Batch 2250/2516 | Avg Loss: 2.9902
Finetune Epoch 2 | Batch 2300/2516 | Avg Loss: 2.9949
Finetune Epoch 2 | Batch 2350/2516 | Avg Loss: 2.9984
Finetune Epoch 2 | Batch 2400/2516 | Avg Loss: 2.9972
Finetune Epoch 2 | Batch 2450/2516 | Avg Loss: 2.9947
Finetune Epoch 2 | Batch 2500/2516 | Avg Loss: 2.9994
--- End of Finetune Epoch 2 | Train Loss: 2.9992 | Val Loss: 3.4931 | Val Acc: 0.0344 | Val F1: 0.2334 | LR: 0.000030 ---
Finetune Epoch 3 | Batch 50/2516 | Avg Loss: 2.6123
Finetune Epoch 3 | Batch 100/2516 | Avg Loss: 2.5721
Finetune Epoch 3 | Batch 150/2516 | Avg Loss: 2.4856
Finetune Epoch 3 | Batch 200/2516 | Avg Loss: 2.4954
Finetune Epoch 3 | Batch 250/2516 | Avg Loss: 2.4775
Finetune Epoch 3 | Batch 300/2516 | Avg Loss: 2.5006
Finetune Epoch 3 | Batch 350/2516 | Avg Loss: 2.5032
Finetune Epoch 3 | Batch 400/2516 | Avg Loss: 2.5303
Finetune Epoch 3 | Batch 450/2516 | Avg Loss: 2.5410
Finetune Epoch 3 | Batch 500/2516 | Avg Loss: 2.5444
Finetune Epoch 3 | Batch 550/2516 | Avg Loss: 2.5444
Finetune Epoch 3 | Batch 600/2516 | Avg Loss: 2.5451
Finetune Epoch 3 | Batch 650/2516 | Avg Loss: 2.5705
Finetune Epoch 3 | Batch 700/2516 | Avg Loss: 2.5796
Finetune Epoch 3 | Batch 750/2516 | Avg Loss: 2.5944
Finetune Epoch 3 | Batch 800/2516 | Avg Loss: 2.5998
Finetune Epoch 3 | Batch 850/2516 | Avg Loss: 2.6073
Finetune Epoch 3 | Batch 900/2516 | Avg Loss: 2.6106
Finetune Epoch 3 | Batch 950/2516 | Avg Loss: 2.6007
Finetune Epoch 3 | Batch 1000/2516 | Avg Loss: 2.5948
Finetune Epoch 3 | Batch 1050/2516 | Avg Loss: 2.5990
Finetune Epoch 3 | Batch 1100/2516 | Avg Loss: 2.6093
Finetune Epoch 3 | Batch 1150/2516 | Avg Loss: 2.6155
Finetune Epoch 3 | Batch 1200/2516 | Avg Loss: 2.6167
Finetune Epoch 3 | Batch 1250/2516 | Avg Loss: 2.6214
Finetune Epoch 3 | Batch 1300/2516 | Avg Loss: 2.6244
Finetune Epoch 3 | Batch 1350/2516 | Avg Loss: 2.6286
Finetune Epoch 3 | Batch 1400/2516 | Avg Loss: 2.6251
Finetune Epoch 3 | Batch 1450/2516 | Avg Loss: 2.6216
Finetune Epoch 3 | Batch 1500/2516 | Avg Loss: 2.6311
Finetune Epoch 3 | Batch 1550/2516 | Avg Loss: 2.6347
Finetune Epoch 3 | Batch 1600/2516 | Avg Loss: 2.6379
Finetune Epoch 3 | Batch 1650/2516 | Avg Loss: 2.6422
Finetune Epoch 3 | Batch 1700/2516 | Avg Loss: 2.6451
Finetune Epoch 3 | Batch 1750/2516 | Avg Loss: 2.6436
Finetune Epoch 3 | Batch 1800/2516 | Avg Loss: 2.6434
Finetune Epoch 3 | Batch 1850/2516 | Avg Loss: 2.6427
Finetune Epoch 3 | Batch 1900/2516 | Avg Loss: 2.6424
Finetune Epoch 3 | Batch 1950/2516 | Avg Loss: 2.6464
Finetune Epoch 3 | Batch 2000/2516 | Avg Loss: 2.6494
Finetune Epoch 3 | Batch 2050/2516 | Avg Loss: 2.6521
Finetune Epoch 3 | Batch 2100/2516 | Avg Loss: 2.6551
Finetune Epoch 3 | Batch 2150/2516 | Avg Loss: 2.6523
Finetune Epoch 3 | Batch 2200/2516 | Avg Loss: 2.6528
Finetune Epoch 3 | Batch 2250/2516 | Avg Loss: 2.6559
Finetune Epoch 3 | Batch 2300/2516 | Avg Loss: 2.6550
Finetune Epoch 3 | Batch 2350/2516 | Avg Loss: 2.6586
Finetune Epoch 3 | Batch 2400/2516 | Avg Loss: 2.6647
Finetune Epoch 3 | Batch 2450/2516 | Avg Loss: 2.6656
Finetune Epoch 3 | Batch 2500/2516 | Avg Loss: 2.6677
--- End of Finetune Epoch 3 | Train Loss: 2.6671 | Val Loss: 3.7288 | Val Acc: 0.0386 | Val F1: 0.2443 | LR: 0.000030 ---
Finetune Epoch 4 | Batch 50/2516 | Avg Loss: 2.2414
Finetune Epoch 4 | Batch 100/2516 | Avg Loss: 2.2618
Finetune Epoch 4 | Batch 150/2516 | Avg Loss: 2.2069
Finetune Epoch 4 | Batch 200/2516 | Avg Loss: 2.1970
Finetune Epoch 4 | Batch 250/2516 | Avg Loss: 2.1928
Finetune Epoch 4 | Batch 300/2516 | Avg Loss: 2.1675
Finetune Epoch 4 | Batch 350/2516 | Avg Loss: 2.1845
Finetune Epoch 4 | Batch 400/2516 | Avg Loss: 2.2174
Finetune Epoch 4 | Batch 450/2516 | Avg Loss: 2.2255
Finetune Epoch 4 | Batch 500/2516 | Avg Loss: 2.2112
Finetune Epoch 4 | Batch 550/2516 | Avg Loss: 2.2201
Finetune Epoch 4 | Batch 600/2516 | Avg Loss: 2.2398
Finetune Epoch 4 | Batch 650/2516 | Avg Loss: 2.2633
Finetune Epoch 4 | Batch 700/2516 | Avg Loss: 2.2751
Finetune Epoch 4 | Batch 750/2516 | Avg Loss: 2.2797
Finetune Epoch 4 | Batch 800/2516 | Avg Loss: 2.2794
Finetune Epoch 4 | Batch 850/2516 | Avg Loss: 2.2763
Finetune Epoch 4 | Batch 900/2516 | Avg Loss: 2.2857
Finetune Epoch 4 | Batch 950/2516 | Avg Loss: 2.2849
Finetune Epoch 4 | Batch 1000/2516 | Avg Loss: 2.2850
Finetune Epoch 4 | Batch 1050/2516 | Avg Loss: 2.2849
Finetune Epoch 4 | Batch 1100/2516 | Avg Loss: 2.2762
Finetune Epoch 4 | Batch 1150/2516 | Avg Loss: 2.2862
Finetune Epoch 4 | Batch 1200/2516 | Avg Loss: 2.2945
Finetune Epoch 4 | Batch 1250/2516 | Avg Loss: 2.2924
Finetune Epoch 4 | Batch 1300/2516 | Avg Loss: 2.2992
Finetune Epoch 4 | Batch 1350/2516 | Avg Loss: 2.2954
Finetune Epoch 4 | Batch 1400/2516 | Avg Loss: 2.3015
Finetune Epoch 4 | Batch 1450/2516 | Avg Loss: 2.2956
Finetune Epoch 4 | Batch 1500/2516 | Avg Loss: 2.2999
Finetune Epoch 4 | Batch 1550/2516 | Avg Loss: 2.2998
Finetune Epoch 4 | Batch 1600/2516 | Avg Loss: 2.3004
Finetune Epoch 4 | Batch 1650/2516 | Avg Loss: 2.2978
Finetune Epoch 4 | Batch 1700/2516 | Avg Loss: 2.3039
Finetune Epoch 4 | Batch 1750/2516 | Avg Loss: 2.3115
Finetune Epoch 4 | Batch 1800/2516 | Avg Loss: 2.3154
Finetune Epoch 4 | Batch 1850/2516 | Avg Loss: 2.3180
Finetune Epoch 4 | Batch 1900/2516 | Avg Loss: 2.3208
Finetune Epoch 4 | Batch 1950/2516 | Avg Loss: 2.3247
Finetune Epoch 4 | Batch 2000/2516 | Avg Loss: 2.3287
Finetune Epoch 4 | Batch 2050/2516 | Avg Loss: 2.3294
Finetune Epoch 4 | Batch 2100/2516 | Avg Loss: 2.3327
Finetune Epoch 4 | Batch 2150/2516 | Avg Loss: 2.3341
Finetune Epoch 4 | Batch 2200/2516 | Avg Loss: 2.3379
Finetune Epoch 4 | Batch 2250/2516 | Avg Loss: 2.3408
Finetune Epoch 4 | Batch 2300/2516 | Avg Loss: 2.3435
Finetune Epoch 4 | Batch 2350/2516 | Avg Loss: 2.3454
Finetune Epoch 4 | Batch 2400/2516 | Avg Loss: 2.3445
Finetune Epoch 4 | Batch 2450/2516 | Avg Loss: 2.3461
Finetune Epoch 4 | Batch 2500/2516 | Avg Loss: 2.3487
--- End of Finetune Epoch 4 | Train Loss: 2.3499 | Val Loss: 4.4102 | Val Acc: 0.0267 | Val F1: 0.2205 | LR: 0.000015 ---
Finetune Epoch 5 | Batch 50/2516 | Avg Loss: 1.9933
Finetune Epoch 5 | Batch 100/2516 | Avg Loss: 2.0059
Finetune Epoch 5 | Batch 150/2516 | Avg Loss: 1.9114
Finetune Epoch 5 | Batch 200/2516 | Avg Loss: 1.8478
Finetune Epoch 5 | Batch 250/2516 | Avg Loss: 1.8435
Finetune Epoch 5 | Batch 300/2516 | Avg Loss: 1.8388
Finetune Epoch 5 | Batch 350/2516 | Avg Loss: 1.8284
Finetune Epoch 5 | Batch 400/2516 | Avg Loss: 1.8559
Finetune Epoch 5 | Batch 450/2516 | Avg Loss: 1.8271
Finetune Epoch 5 | Batch 500/2516 | Avg Loss: 1.8593
Finetune Epoch 5 | Batch 550/2516 | Avg Loss: 1.8504
Finetune Epoch 5 | Batch 600/2516 | Avg Loss: 1.8424
Finetune Epoch 5 | Batch 650/2516 | Avg Loss: 1.8424
Finetune Epoch 5 | Batch 700/2516 | Avg Loss: 1.8711
Finetune Epoch 5 | Batch 750/2516 | Avg Loss: 1.8629
Finetune Epoch 5 | Batch 800/2516 | Avg Loss: 1.8721
Finetune Epoch 5 | Batch 850/2516 | Avg Loss: 1.8664
Finetune Epoch 5 | Batch 900/2516 | Avg Loss: 1.8627
Finetune Epoch 5 | Batch 950/2516 | Avg Loss: 1.8603
Finetune Epoch 5 | Batch 1000/2516 | Avg Loss: 1.8599
Finetune Epoch 5 | Batch 1050/2516 | Avg Loss: 1.8521
Finetune Epoch 5 | Batch 1100/2516 | Avg Loss: 1.8490
Finetune Epoch 5 | Batch 1150/2516 | Avg Loss: 1.8594
Finetune Epoch 5 | Batch 1200/2516 | Avg Loss: 1.8660
Finetune Epoch 5 | Batch 1250/2516 | Avg Loss: 1.8752
Finetune Epoch 5 | Batch 1300/2516 | Avg Loss: 1.8766
Finetune Epoch 5 | Batch 1350/2516 | Avg Loss: 1.8765
Finetune Epoch 5 | Batch 1400/2516 | Avg Loss: 1.8779
Finetune Epoch 5 | Batch 1450/2516 | Avg Loss: 1.8802
Finetune Epoch 5 | Batch 1500/2516 | Avg Loss: 1.8756
Finetune Epoch 5 | Batch 1550/2516 | Avg Loss: 1.8862
Finetune Epoch 5 | Batch 1600/2516 | Avg Loss: 1.8861
Finetune Epoch 5 | Batch 1650/2516 | Avg Loss: 1.8839
Finetune Epoch 5 | Batch 1700/2516 | Avg Loss: 1.8843
Finetune Epoch 5 | Batch 1750/2516 | Avg Loss: 1.8879
Finetune Epoch 5 | Batch 1800/2516 | Avg Loss: 1.8929
Finetune Epoch 5 | Batch 1850/2516 | Avg Loss: 1.8924
Finetune Epoch 5 | Batch 1900/2516 | Avg Loss: 1.8902
Finetune Epoch 5 | Batch 1950/2516 | Avg Loss: 1.8920
Finetune Epoch 5 | Batch 2000/2516 | Avg Loss: 1.8939
Finetune Epoch 5 | Batch 2050/2516 | Avg Loss: 1.8913
Finetune Epoch 5 | Batch 2100/2516 | Avg Loss: 1.8923
Finetune Epoch 5 | Batch 2150/2516 | Avg Loss: 1.8925
Finetune Epoch 5 | Batch 2200/2516 | Avg Loss: 1.8960
Finetune Epoch 5 | Batch 2250/2516 | Avg Loss: 1.8991
Finetune Epoch 5 | Batch 2300/2516 | Avg Loss: 1.9015
Finetune Epoch 5 | Batch 2350/2516 | Avg Loss: 1.9067
Finetune Epoch 5 | Batch 2400/2516 | Avg Loss: 1.9090
Finetune Epoch 5 | Batch 2450/2516 | Avg Loss: 1.9087
Finetune Epoch 5 | Batch 2500/2516 | Avg Loss: 1.9082
--- End of Finetune Epoch 5 | Train Loss: 1.9097 | Val Loss: 5.2840 | Val Acc: 0.0314 | Val F1: 0.2030 | LR: 0.000015 ---
Finetune Epoch 6 | Batch 50/2516 | Avg Loss: 1.4272
Finetune Epoch 6 | Batch 100/2516 | Avg Loss: 1.4670
Finetune Epoch 6 | Batch 150/2516 | Avg Loss: 1.5313
Finetune Epoch 6 | Batch 200/2516 | Avg Loss: 1.5089
Finetune Epoch 6 | Batch 250/2516 | Avg Loss: 1.5407
Finetune Epoch 6 | Batch 300/2516 | Avg Loss: 1.5579
Finetune Epoch 6 | Batch 350/2516 | Avg Loss: 1.5391
Finetune Epoch 6 | Batch 400/2516 | Avg Loss: 1.5609
Finetune Epoch 6 | Batch 450/2516 | Avg Loss: 1.5783
Finetune Epoch 6 | Batch 500/2516 | Avg Loss: 1.5938
Finetune Epoch 6 | Batch 550/2516 | Avg Loss: 1.6105
Finetune Epoch 6 | Batch 600/2516 | Avg Loss: 1.6264
Finetune Epoch 6 | Batch 650/2516 | Avg Loss: 1.6362
Finetune Epoch 6 | Batch 700/2516 | Avg Loss: 1.6304
Finetune Epoch 6 | Batch 750/2516 | Avg Loss: 1.6470
Finetune Epoch 6 | Batch 800/2516 | Avg Loss: 1.6614
Finetune Epoch 6 | Batch 850/2516 | Avg Loss: 1.6697
Finetune Epoch 6 | Batch 900/2516 | Avg Loss: 1.6870
Finetune Epoch 6 | Batch 950/2516 | Avg Loss: 1.6920
Finetune Epoch 6 | Batch 1000/2516 | Avg Loss: 1.6851
Finetune Epoch 6 | Batch 1050/2516 | Avg Loss: 1.6851
Finetune Epoch 6 | Batch 1100/2516 | Avg Loss: 1.6827
Finetune Epoch 6 | Batch 1150/2516 | Avg Loss: 1.6853
Finetune Epoch 6 | Batch 1200/2516 | Avg Loss: 1.7001
Finetune Epoch 6 | Batch 1250/2516 | Avg Loss: 1.6977
Finetune Epoch 6 | Batch 1300/2516 | Avg Loss: 1.7045
Finetune Epoch 6 | Batch 1350/2516 | Avg Loss: 1.7059
Finetune Epoch 6 | Batch 1400/2516 | Avg Loss: 1.7079
Finetune Epoch 6 | Batch 1450/2516 | Avg Loss: 1.7123
Finetune Epoch 6 | Batch 1500/2516 | Avg Loss: 1.7128
Finetune Epoch 6 | Batch 1550/2516 | Avg Loss: 1.7075
Finetune Epoch 6 | Batch 1600/2516 | Avg Loss: 1.7040
Finetune Epoch 6 | Batch 1650/2516 | Avg Loss: 1.7033
Finetune Epoch 6 | Batch 1700/2516 | Avg Loss: 1.7068
Finetune Epoch 6 | Batch 1750/2516 | Avg Loss: 1.7099
Finetune Epoch 6 | Batch 1800/2516 | Avg Loss: 1.7067
Finetune Epoch 6 | Batch 1850/2516 | Avg Loss: 1.7081
Finetune Epoch 6 | Batch 1900/2516 | Avg Loss: 1.7122
Finetune Epoch 6 | Batch 1950/2516 | Avg Loss: 1.7131
Finetune Epoch 6 | Batch 2000/2516 | Avg Loss: 1.7132
Finetune Epoch 6 | Batch 2050/2516 | Avg Loss: 1.7167
Finetune Epoch 6 | Batch 2100/2516 | Avg Loss: 1.7167
Finetune Epoch 6 | Batch 2150/2516 | Avg Loss: 1.7170
Finetune Epoch 6 | Batch 2200/2516 | Avg Loss: 1.7202
Finetune Epoch 6 | Batch 2250/2516 | Avg Loss: 1.7258
Finetune Epoch 6 | Batch 2300/2516 | Avg Loss: 1.7262
Finetune Epoch 6 | Batch 2350/2516 | Avg Loss: 1.7260
Finetune Epoch 6 | Batch 2400/2516 | Avg Loss: 1.7280
Finetune Epoch 6 | Batch 2450/2516 | Avg Loss: 1.7308
Finetune Epoch 6 | Batch 2500/2516 | Avg Loss: 1.7271
--- End of Finetune Epoch 6 | Train Loss: 1.7273 | Val Loss: 5.7945 | Val Acc: 0.0285 | Val F1: 0.1992 | LR: 0.000015 ---

Fine-tuning finished.
Best fine-tuned QA model state_dict saved to models/BaseQA_20250915-112724/toy_llm_qasrl_finetuned.pth
Loading best model from models/BaseQA_20250915-112724/toy_llm_qasrl_finetuned.pth for final stats.
Final Validation Accuracy: 0.0480 | Final F1: 0.1900
Final training statistics saved to 'models/BaseQA_20250915-112724/finetune_stats.json'
Script finished.
