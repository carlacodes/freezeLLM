Thu Aug 28 13:53:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:58:00.0 Off |                    0 |
| N/A   35C    P0             37W /  250W |       1MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
--- Loaded configuration 'small' from '/home/zceccgr/Scratch/freezeLLM/llm_scale_up/config.json' ---
Using device: cuda
Loading standard tokenizer ('bert-base-uncased')...
Standard vocabulary size: 30522
PAD token ID: 0
Instantiated Base Model: SmallQA with 11,038,720 trainable parameters.
Best pre-trained model will be saved to: models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth

--- Starting CLM Pre-training on nq_open for 'SmallQA' model ---
--- End of Pre-train Epoch 1 | Train Loss: 5.8819 | Validation Loss: 5.5644 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 2 | Train Loss: 5.2049 | Validation Loss: 5.3123 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 3 | Train Loss: 4.9356 | Validation Loss: 5.1891 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 4 | Train Loss: 4.7710 | Validation Loss: 5.1176 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 5 | Train Loss: 4.6577 | Validation Loss: 5.0896 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 6 | Train Loss: 4.5771 | Validation Loss: 5.0683 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 7 | Train Loss: 4.5147 | Validation Loss: 5.0260 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 8 | Train Loss: 4.4674 | Validation Loss: 5.0156 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 9 | Train Loss: 4.4281 | Validation Loss: 5.0052 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 10 | Train Loss: 4.3955 | Validation Loss: 5.0049 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 11 | Train Loss: 4.3690 | Validation Loss: 5.0040 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 12 | Train Loss: 4.3447 | Validation Loss: 4.9884 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 13 | Train Loss: 4.3250 | Validation Loss: 4.9788 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 14 | Train Loss: 4.3074 | Validation Loss: 4.9752 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 15 | Train Loss: 4.2896 | Validation Loss: 4.9849 | LR: 0.000299 ---
--- End of Pre-train Epoch 16 | Train Loss: 4.2743 | Validation Loss: 4.9758 | LR: 0.000299 ---
--- End of Pre-train Epoch 17 | Train Loss: 4.2647 | Validation Loss: 4.9641 | LR: 0.000299 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 18 | Train Loss: 4.2519 | Validation Loss: 4.9698 | LR: 0.000299 ---
--- End of Pre-train Epoch 19 | Train Loss: 4.2409 | Validation Loss: 4.9649 | LR: 0.000299 ---
--- End of Pre-train Epoch 20 | Train Loss: 4.2305 | Validation Loss: 4.9679 | LR: 0.000299 ---
--- End of Pre-train Epoch 21 | Train Loss: 3.9547 | Validation Loss: 4.8826 | LR: 0.000150 ---
Validation loss improved. Saving model to models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 22 | Train Loss: 3.8721 | Validation Loss: 4.8997 | LR: 0.000150 ---
--- End of Pre-train Epoch 23 | Train Loss: 3.8509 | Validation Loss: 4.9095 | LR: 0.000150 ---
--- End of Pre-train Epoch 24 | Train Loss: 3.8443 | Validation Loss: 4.9214 | LR: 0.000150 ---
--- End of Pre-train Epoch 25 | Train Loss: 3.6499 | Validation Loss: 4.8865 | LR: 0.000075 ---
--- End of Pre-train Epoch 26 | Train Loss: 3.6000 | Validation Loss: 4.9024 | LR: 0.000075 ---

Early stopping triggered after 5 epochs without improvement.

Loading best pretrained model from models/SmallQA_20250828-135428/toy_llm_unified_pretrained.pth

Testing prompt completion after pretraining:
Prompt: 'The capital of France is'
Model completion: the capital of france is ruling in combine bygre rutherford, australia

--- Starting Fine-tuning on QA-SRL for 'SmallQA' model ---
Successfully loaded pre-trained weights into the QA model.
Loading and processing qa_srl dataset for 'train' split...
Finished processing 'train'. Found 7562 valid QA examples.
Starting fine-tuning for 3 epochs...
Finetune Epoch 1 | Batch 50/1891 | Avg Loss: 4.4666
Finetune Epoch 1 | Batch 100/1891 | Avg Loss: 4.0937
Finetune Epoch 1 | Batch 150/1891 | Avg Loss: 3.9128
Finetune Epoch 1 | Batch 200/1891 | Avg Loss: 3.7962
Finetune Epoch 1 | Batch 250/1891 | Avg Loss: 3.6979
Finetune Epoch 1 | Batch 300/1891 | Avg Loss: 3.6251
Finetune Epoch 1 | Batch 350/1891 | Avg Loss: 3.5662
Finetune Epoch 1 | Batch 400/1891 | Avg Loss: 3.5176
Finetune Epoch 1 | Batch 450/1891 | Avg Loss: 3.4767
Finetune Epoch 1 | Batch 500/1891 | Avg Loss: 3.4365
Finetune Epoch 1 | Batch 550/1891 | Avg Loss: 3.4127
Finetune Epoch 1 | Batch 600/1891 | Avg Loss: 3.3872
Finetune Epoch 1 | Batch 650/1891 | Avg Loss: 3.3596
Finetune Epoch 1 | Batch 700/1891 | Avg Loss: 3.3371
Finetune Epoch 1 | Batch 750/1891 | Avg Loss: 3.3123
Finetune Epoch 1 | Batch 800/1891 | Avg Loss: 3.2842
Finetune Epoch 1 | Batch 850/1891 | Avg Loss: 3.2689
Finetune Epoch 1 | Batch 900/1891 | Avg Loss: 3.2566
Finetune Epoch 1 | Batch 950/1891 | Avg Loss: 3.2457
Finetune Epoch 1 | Batch 1000/1891 | Avg Loss: 3.2327
Finetune Epoch 1 | Batch 1050/1891 | Avg Loss: 3.2191
Finetune Epoch 1 | Batch 1100/1891 | Avg Loss: 3.2044
Finetune Epoch 1 | Batch 1150/1891 | Avg Loss: 3.1935
Finetune Epoch 1 | Batch 1200/1891 | Avg Loss: 3.1810
Finetune Epoch 1 | Batch 1250/1891 | Avg Loss: 3.1698
Finetune Epoch 1 | Batch 1300/1891 | Avg Loss: 3.1590
Finetune Epoch 1 | Batch 1350/1891 | Avg Loss: 3.1488
Finetune Epoch 1 | Batch 1400/1891 | Avg Loss: 3.1432
Finetune Epoch 1 | Batch 1450/1891 | Avg Loss: 3.1306
Finetune Epoch 1 | Batch 1500/1891 | Avg Loss: 3.1215
Finetune Epoch 1 | Batch 1550/1891 | Avg Loss: 3.1090
Finetune Epoch 1 | Batch 1600/1891 | Avg Loss: 3.0968
Finetune Epoch 1 | Batch 1650/1891 | Avg Loss: 3.0850
Finetune Epoch 1 | Batch 1700/1891 | Avg Loss: 3.0786
Finetune Epoch 1 | Batch 1750/1891 | Avg Loss: 3.0688
Finetune Epoch 1 | Batch 1800/1891 | Avg Loss: 3.0614
Finetune Epoch 1 | Batch 1850/1891 | Avg Loss: 3.0525
--- End of Finetune Epoch 1 | Average QA Loss: 3.0453 | LR: 0.000050 ---
Finetune Epoch 2 | Batch 50/1891 | Avg Loss: 2.4130
Finetune Epoch 2 | Batch 100/1891 | Avg Loss: 2.4359
Finetune Epoch 2 | Batch 150/1891 | Avg Loss: 2.4775
Finetune Epoch 2 | Batch 200/1891 | Avg Loss: 2.4476
Finetune Epoch 2 | Batch 250/1891 | Avg Loss: 2.4777
Finetune Epoch 2 | Batch 300/1891 | Avg Loss: 2.4837
Finetune Epoch 2 | Batch 350/1891 | Avg Loss: 2.4657
Finetune Epoch 2 | Batch 400/1891 | Avg Loss: 2.4662
Finetune Epoch 2 | Batch 450/1891 | Avg Loss: 2.4750
Finetune Epoch 2 | Batch 500/1891 | Avg Loss: 2.4811
Finetune Epoch 2 | Batch 550/1891 | Avg Loss: 2.4796
Finetune Epoch 2 | Batch 600/1891 | Avg Loss: 2.4782
Finetune Epoch 2 | Batch 650/1891 | Avg Loss: 2.4720
Finetune Epoch 2 | Batch 700/1891 | Avg Loss: 2.4605
Finetune Epoch 2 | Batch 750/1891 | Avg Loss: 2.4517
Finetune Epoch 2 | Batch 800/1891 | Avg Loss: 2.4534
Finetune Epoch 2 | Batch 850/1891 | Avg Loss: 2.4467
Finetune Epoch 2 | Batch 900/1891 | Avg Loss: 2.4549
Finetune Epoch 2 | Batch 950/1891 | Avg Loss: 2.4565
Finetune Epoch 2 | Batch 1000/1891 | Avg Loss: 2.4574
Finetune Epoch 2 | Batch 1050/1891 | Avg Loss: 2.4591
Finetune Epoch 2 | Batch 1100/1891 | Avg Loss: 2.4565
Finetune Epoch 2 | Batch 1150/1891 | Avg Loss: 2.4605
Finetune Epoch 2 | Batch 1200/1891 | Avg Loss: 2.4627
Finetune Epoch 2 | Batch 1250/1891 | Avg Loss: 2.4560
Finetune Epoch 2 | Batch 1300/1891 | Avg Loss: 2.4547
Finetune Epoch 2 | Batch 1350/1891 | Avg Loss: 2.4534
Finetune Epoch 2 | Batch 1400/1891 | Avg Loss: 2.4537
Finetune Epoch 2 | Batch 1450/1891 | Avg Loss: 2.4541
Finetune Epoch 2 | Batch 1500/1891 | Avg Loss: 2.4569
Finetune Epoch 2 | Batch 1550/1891 | Avg Loss: 2.4557
Finetune Epoch 2 | Batch 1600/1891 | Avg Loss: 2.4562
Finetune Epoch 2 | Batch 1650/1891 | Avg Loss: 2.4552
Finetune Epoch 2 | Batch 1700/1891 | Avg Loss: 2.4500
Finetune Epoch 2 | Batch 1750/1891 | Avg Loss: 2.4520
Finetune Epoch 2 | Batch 1800/1891 | Avg Loss: 2.4481
Finetune Epoch 2 | Batch 1850/1891 | Avg Loss: 2.4454
--- End of Finetune Epoch 2 | Average QA Loss: 2.4428 | LR: 0.000050 ---
Finetune Epoch 3 | Batch 50/1891 | Avg Loss: 2.1085
Finetune Epoch 3 | Batch 100/1891 | Avg Loss: 2.0780
Finetune Epoch 3 | Batch 150/1891 | Avg Loss: 2.0607
Finetune Epoch 3 | Batch 200/1891 | Avg Loss: 2.0590
Finetune Epoch 3 | Batch 250/1891 | Avg Loss: 2.0505
Finetune Epoch 3 | Batch 300/1891 | Avg Loss: 2.0637
Finetune Epoch 3 | Batch 350/1891 | Avg Loss: 2.0628
Finetune Epoch 3 | Batch 400/1891 | Avg Loss: 2.0757
Finetune Epoch 3 | Batch 450/1891 | Avg Loss: 2.0812
Finetune Epoch 3 | Batch 500/1891 | Avg Loss: 2.0801
Finetune Epoch 3 | Batch 550/1891 | Avg Loss: 2.0840
Finetune Epoch 3 | Batch 600/1891 | Avg Loss: 2.0736
Finetune Epoch 3 | Batch 650/1891 | Avg Loss: 2.0655
Finetune Epoch 3 | Batch 700/1891 | Avg Loss: 2.0694
Finetune Epoch 3 | Batch 750/1891 | Avg Loss: 2.0708
Finetune Epoch 3 | Batch 800/1891 | Avg Loss: 2.0716
Finetune Epoch 3 | Batch 850/1891 | Avg Loss: 2.0715
Finetune Epoch 3 | Batch 900/1891 | Avg Loss: 2.0784
Finetune Epoch 3 | Batch 950/1891 | Avg Loss: 2.0809
Finetune Epoch 3 | Batch 1000/1891 | Avg Loss: 2.0798
Finetune Epoch 3 | Batch 1050/1891 | Avg Loss: 2.0818
Finetune Epoch 3 | Batch 1100/1891 | Avg Loss: 2.0815
Finetune Epoch 3 | Batch 1150/1891 | Avg Loss: 2.0835
Finetune Epoch 3 | Batch 1200/1891 | Avg Loss: 2.0831
Finetune Epoch 3 | Batch 1250/1891 | Avg Loss: 2.0899
Finetune Epoch 3 | Batch 1300/1891 | Avg Loss: 2.0931
Finetune Epoch 3 | Batch 1350/1891 | Avg Loss: 2.0908
Finetune Epoch 3 | Batch 1400/1891 | Avg Loss: 2.0899
Finetune Epoch 3 | Batch 1450/1891 | Avg Loss: 2.0924
Finetune Epoch 3 | Batch 1500/1891 | Avg Loss: 2.0914
Finetune Epoch 3 | Batch 1550/1891 | Avg Loss: 2.0899
Finetune Epoch 3 | Batch 1600/1891 | Avg Loss: 2.0906
Finetune Epoch 3 | Batch 1650/1891 | Avg Loss: 2.0889
Finetune Epoch 3 | Batch 1700/1891 | Avg Loss: 2.0894
Finetune Epoch 3 | Batch 1750/1891 | Avg Loss: 2.0890
Finetune Epoch 3 | Batch 1800/1891 | Avg Loss: 2.0862
Finetune Epoch 3 | Batch 1850/1891 | Avg Loss: 2.0870
--- End of Finetune Epoch 3 | Average QA Loss: 2.0889 | LR: 0.000050 ---

Fine-tuning finished.
Fine-tuned QA model state_dict saved to models/SmallQA_20250828-135428/toy_llm_qasrl_finetuned.pth
Final Training Set Accuracy: 0.2831 | Final F1: 0.4493
Final training statistics saved to 'models/SmallQA_20250828-135428/finetune_stats.json'

--- Running Validation Metrics on QA-SRL Validation Set for 'SmallQA' model ---
Loaded finetuned model from models/SmallQA_20250828-135428/toy_llm_qasrl_finetuned.pth for validation.
Loading and processing qa_srl dataset for 'validation' split...
Finished processing 'validation'. Found 2571 valid QA examples.
Final Validation Accuracy: 0.1229 | Final Validation F1: 0.2868
Validation statistics saved to 'models/SmallQA_20250828-135428/validation_stats.json'
