{
    "tiny": {
      "llm_config": {
        "name": "TinyQA",
        "n_layers": 2,
        "hidden_size": 128,
        "n_heads": 2
      },
      "training_params": {
        "max_seq_len": 256,
        "dropout_rate": 0.1,
        "pretrain_lr": 3e-4,
        "num_pretrain_epochs": 100,
        "pretrain_patience": 5,
        "warmup_steps": 500,
        "batch_size_pretrain": 16,
        "num_finetune_epochs": 6,
        "finetune_lr": 5e-5,
        "batch_size_qa": 8
      }
    },
    "small": {
      "llm_config": {
        "name": "SmallQA",
        "n_layers": 4,
        "hidden_size": 256,
        "n_heads": 4
      },
      "training_params": {
        "max_seq_len": 256,
        "dropout_rate": 0.1,
        "pretrain_lr": 3e-4,
        "num_pretrain_epochs": 100,
        "pretrain_patience": 5,
        "warmup_steps": 500,
        "batch_size_pretrain": 8,
        "num_finetune_epochs": 3,
        "finetune_lr": 5e-5,
        "batch_size_qa": 4
      }
    },
    "base": {
      "llm_config": {
        "name": "BaseQA",
        "n_layers": 6,
        "hidden_size": 512,
        "n_heads": 8
      },
      "training_params": {
        "max_seq_len": 512,
        "dropout_rate": 0.1,
        "pretrain_lr": 1e-5,
        "num_pretrain_epochs": 100,
        "pretrain_patience": 5,
        "warmup_steps": 1000,
        "batch_size_pretrain": 4,
        "num_finetune_epochs": 6,
        "finetune_lr": 3e-5,
        "batch_size_qa": 2
      }
    }
  }