Mon Sep 15 02:46:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:58:00.0 Off |                    0 |
| N/A   35C    P0             37W /  250W |       1MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Starting Python script for smallqa model...
--- Loaded configuration 'tiny' from '/home/zceccgr/Scratch/freezeLLM/llm_scale_up/config.json' ---
Using device: cuda
Loading standard tokenizer ('bert-base-uncased')...
Standard vocabulary size: 30522
PAD token ID: 0
Instantiated Base Model: TinyQA with 4,336,384 trainable parameters.
Best pre-trained model will be saved to: models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth

--- Starting CLM Pre-training on nq_open for 'TinyQA' model ---
--- End of Pre-train Epoch 1 | Train Loss: 5.1998 | Validation Loss: 4.8190 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 2 | Train Loss: 4.5765 | Validation Loss: 4.6161 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 3 | Train Loss: 4.3796 | Validation Loss: 4.5094 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 4 | Train Loss: 4.2446 | Validation Loss: 4.4449 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 5 | Train Loss: 4.1447 | Validation Loss: 4.4016 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 6 | Train Loss: 4.0697 | Validation Loss: 4.3692 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 7 | Train Loss: 4.0124 | Validation Loss: 4.3596 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 8 | Train Loss: 3.9676 | Validation Loss: 4.3405 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 9 | Train Loss: 3.9318 | Validation Loss: 4.3300 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 10 | Train Loss: 3.9039 | Validation Loss: 4.3169 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 11 | Train Loss: 3.8805 | Validation Loss: 4.3094 | LR: 0.000299 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 12 | Train Loss: 3.8614 | Validation Loss: 4.3135 | LR: 0.000299 ---
--- End of Pre-train Epoch 13 | Train Loss: 3.8461 | Validation Loss: 4.3095 | LR: 0.000299 ---
--- End of Pre-train Epoch 14 | Train Loss: 3.8329 | Validation Loss: 4.3104 | LR: 0.000299 ---
--- End of Pre-train Epoch 15 | Train Loss: 3.7192 | Validation Loss: 4.2762 | LR: 0.000150 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 16 | Train Loss: 3.6909 | Validation Loss: 4.2747 | LR: 0.000150 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 17 | Train Loss: 3.6765 | Validation Loss: 4.2708 | LR: 0.000150 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 18 | Train Loss: 3.6676 | Validation Loss: 4.2687 | LR: 0.000150 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 19 | Train Loss: 3.6614 | Validation Loss: 4.2698 | LR: 0.000150 ---
--- End of Pre-train Epoch 20 | Train Loss: 3.6568 | Validation Loss: 4.2717 | LR: 0.000150 ---
--- End of Pre-train Epoch 21 | Train Loss: 3.6528 | Validation Loss: 4.2685 | LR: 0.000150 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 22 | Train Loss: 3.5820 | Validation Loss: 4.2536 | LR: 0.000075 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 23 | Train Loss: 3.5664 | Validation Loss: 4.2539 | LR: 0.000075 ---
--- End of Pre-train Epoch 24 | Train Loss: 3.5603 | Validation Loss: 4.2543 | LR: 0.000075 ---
--- End of Pre-train Epoch 25 | Train Loss: 3.5541 | Validation Loss: 4.2541 | LR: 0.000075 ---
--- End of Pre-train Epoch 26 | Train Loss: 3.5141 | Validation Loss: 4.2471 | LR: 0.000037 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 27 | Train Loss: 3.5062 | Validation Loss: 4.2468 | LR: 0.000037 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 28 | Train Loss: 3.5004 | Validation Loss: 4.2477 | LR: 0.000037 ---
--- End of Pre-train Epoch 29 | Train Loss: 3.4976 | Validation Loss: 4.2479 | LR: 0.000037 ---
--- End of Pre-train Epoch 30 | Train Loss: 3.4742 | Validation Loss: 4.2457 | LR: 0.000019 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 31 | Train Loss: 3.4707 | Validation Loss: 4.2454 | LR: 0.000019 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 32 | Train Loss: 3.4669 | Validation Loss: 4.2458 | LR: 0.000019 ---
--- End of Pre-train Epoch 33 | Train Loss: 3.4650 | Validation Loss: 4.2444 | LR: 0.000019 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 34 | Train Loss: 3.4633 | Validation Loss: 4.2450 | LR: 0.000019 ---
--- End of Pre-train Epoch 35 | Train Loss: 3.4604 | Validation Loss: 4.2458 | LR: 0.000019 ---
--- End of Pre-train Epoch 36 | Train Loss: 3.4586 | Validation Loss: 4.2459 | LR: 0.000019 ---
--- End of Pre-train Epoch 37 | Train Loss: 3.4475 | Validation Loss: 4.2435 | LR: 0.000009 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 38 | Train Loss: 3.4460 | Validation Loss: 4.2442 | LR: 0.000009 ---
--- End of Pre-train Epoch 39 | Train Loss: 3.4440 | Validation Loss: 4.2439 | LR: 0.000009 ---
--- End of Pre-train Epoch 40 | Train Loss: 3.4433 | Validation Loss: 4.2435 | LR: 0.000009 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 41 | Train Loss: 3.4373 | Validation Loss: 4.2427 | LR: 0.000005 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 42 | Train Loss: 3.4361 | Validation Loss: 4.2424 | LR: 0.000005 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 43 | Train Loss: 3.4359 | Validation Loss: 4.2424 | LR: 0.000005 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 44 | Train Loss: 3.4350 | Validation Loss: 4.2423 | LR: 0.000005 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 45 | Train Loss: 3.4334 | Validation Loss: 4.2428 | LR: 0.000005 ---
--- End of Pre-train Epoch 46 | Train Loss: 3.4335 | Validation Loss: 4.2422 | LR: 0.000005 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 47 | Train Loss: 3.4329 | Validation Loss: 4.2425 | LR: 0.000005 ---
--- End of Pre-train Epoch 48 | Train Loss: 3.4296 | Validation Loss: 4.2418 | LR: 0.000002 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 49 | Train Loss: 3.4298 | Validation Loss: 4.2418 | LR: 0.000002 ---
--- End of Pre-train Epoch 50 | Train Loss: 3.4294 | Validation Loss: 4.2420 | LR: 0.000002 ---
--- End of Pre-train Epoch 51 | Train Loss: 3.4289 | Validation Loss: 4.2422 | LR: 0.000002 ---
--- End of Pre-train Epoch 52 | Train Loss: 3.4267 | Validation Loss: 4.2415 | LR: 0.000001 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 53 | Train Loss: 3.4269 | Validation Loss: 4.2413 | LR: 0.000001 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 54 | Train Loss: 3.4265 | Validation Loss: 4.2414 | LR: 0.000001 ---
--- End of Pre-train Epoch 55 | Train Loss: 3.4263 | Validation Loss: 4.2413 | LR: 0.000001 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 56 | Train Loss: 3.4265 | Validation Loss: 4.2411 | LR: 0.000001 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 57 | Train Loss: 3.4249 | Validation Loss: 4.2409 | LR: 0.000001 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 58 | Train Loss: 3.4255 | Validation Loss: 4.2409 | LR: 0.000001 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 59 | Train Loss: 3.4258 | Validation Loss: 4.2407 | LR: 0.000001 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 60 | Train Loss: 3.4252 | Validation Loss: 4.2404 | LR: 0.000001 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 61 | Train Loss: 3.4258 | Validation Loss: 4.2402 | LR: 0.000001 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 62 | Train Loss: 3.4254 | Validation Loss: 4.2401 | LR: 0.000001 ---
Validation loss improved. Saving model to models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth
--- End of Pre-train Epoch 63 | Train Loss: 3.4251 | Validation Loss: 4.2401 | LR: 0.000001 ---
--- End of Pre-train Epoch 64 | Train Loss: 3.4251 | Validation Loss: 4.2405 | LR: 0.000000 ---
--- End of Pre-train Epoch 65 | Train Loss: 3.4248 | Validation Loss: 4.2408 | LR: 0.000000 ---
--- End of Pre-train Epoch 66 | Train Loss: 3.4241 | Validation Loss: 4.2411 | LR: 0.000000 ---
--- End of Pre-train Epoch 67 | Train Loss: 3.4248 | Validation Loss: 4.2412 | LR: 0.000000 ---

Early stopping triggered after 5 epochs without improvement.

Loading best pretrained model from models/TinyQA_20250915-024709/toy_llm_unified_pretrained.pth

Testing prompt completion after pretraining:
Prompt: 'Question: where was the statue of liberty originally built'
Model completion: question : where was the statue of liberty originally built in the philippines answer : " ruled by an commonly

--- Starting Fine-tuning on QA-SRL for 'TinyQA' model ---
Successfully loaded pre-trained weights into the QA model.
Loading and processing qa_srl dataset for 'train' split...
Finished processing 'train'. Found 5031 valid QA examples.
Loading and processing qa_srl dataset for 'validation' split...
Finished processing 'validation'. Found 1686 valid QA examples.
Starting fine-tuning for 6 epochs...
Finetune Epoch 1 | Batch 50/629 | Avg Loss: 4.8350
Finetune Epoch 1 | Batch 100/629 | Avg Loss: 4.5548
Finetune Epoch 1 | Batch 150/629 | Avg Loss: 4.3221
Finetune Epoch 1 | Batch 200/629 | Avg Loss: 4.1471
Finetune Epoch 1 | Batch 250/629 | Avg Loss: 4.0483
Finetune Epoch 1 | Batch 300/629 | Avg Loss: 3.9702
Finetune Epoch 1 | Batch 350/629 | Avg Loss: 3.9012
Finetune Epoch 1 | Batch 400/629 | Avg Loss: 3.8463
Finetune Epoch 1 | Batch 450/629 | Avg Loss: 3.7982
Finetune Epoch 1 | Batch 500/629 | Avg Loss: 3.7647
Finetune Epoch 1 | Batch 550/629 | Avg Loss: 3.7265
Finetune Epoch 1 | Batch 600/629 | Avg Loss: 3.6971
--- End of Finetune Epoch 1 | Train Loss: 3.6773 | Val Loss: 3.3557 | Val Acc: 0.0101 | Val F1: 0.1781 | LR: 0.000050 ---
Validation loss improved. Saving best model to models/TinyQA_20250915-024709/toy_llm_qasrl_finetuned.pth
Finetune Epoch 2 | Batch 50/629 | Avg Loss: 3.2163
Finetune Epoch 2 | Batch 100/629 | Avg Loss: 3.2159
Finetune Epoch 2 | Batch 150/629 | Avg Loss: 3.2145
Finetune Epoch 2 | Batch 200/629 | Avg Loss: 3.2120
Finetune Epoch 2 | Batch 250/629 | Avg Loss: 3.2185
Finetune Epoch 2 | Batch 300/629 | Avg Loss: 3.2182
Finetune Epoch 2 | Batch 350/629 | Avg Loss: 3.2186
Finetune Epoch 2 | Batch 400/629 | Avg Loss: 3.2177
Finetune Epoch 2 | Batch 450/629 | Avg Loss: 3.2182
Finetune Epoch 2 | Batch 500/629 | Avg Loss: 3.2135
Finetune Epoch 2 | Batch 550/629 | Avg Loss: 3.2114
Finetune Epoch 2 | Batch 600/629 | Avg Loss: 3.2105
--- End of Finetune Epoch 2 | Train Loss: 3.2085 | Val Loss: 3.3771 | Val Acc: 0.0089 | Val F1: 0.1845 | LR: 0.000050 ---
Finetune Epoch 3 | Batch 50/629 | Avg Loss: 3.0366
Finetune Epoch 3 | Batch 100/629 | Avg Loss: 3.0363
Finetune Epoch 3 | Batch 150/629 | Avg Loss: 3.0210
Finetune Epoch 3 | Batch 200/629 | Avg Loss: 3.0487
Finetune Epoch 3 | Batch 250/629 | Avg Loss: 3.0581
Finetune Epoch 3 | Batch 300/629 | Avg Loss: 3.0478
Finetune Epoch 3 | Batch 350/629 | Avg Loss: 3.0455
Finetune Epoch 3 | Batch 400/629 | Avg Loss: 3.0467
Finetune Epoch 3 | Batch 450/629 | Avg Loss: 3.0394
Finetune Epoch 3 | Batch 500/629 | Avg Loss: 3.0408
Finetune Epoch 3 | Batch 550/629 | Avg Loss: 3.0430
Finetune Epoch 3 | Batch 600/629 | Avg Loss: 3.0390
--- End of Finetune Epoch 3 | Train Loss: 3.0356 | Val Loss: 3.5220 | Val Acc: 0.0202 | Val F1: 0.1871 | LR: 0.000050 ---
Finetune Epoch 4 | Batch 50/629 | Avg Loss: 2.8776
Finetune Epoch 4 | Batch 100/629 | Avg Loss: 2.8473
Finetune Epoch 4 | Batch 150/629 | Avg Loss: 2.8530
Finetune Epoch 4 | Batch 200/629 | Avg Loss: 2.8467
Finetune Epoch 4 | Batch 250/629 | Avg Loss: 2.8502
Finetune Epoch 4 | Batch 300/629 | Avg Loss: 2.8595
Finetune Epoch 4 | Batch 350/629 | Avg Loss: 2.8611
Finetune Epoch 4 | Batch 400/629 | Avg Loss: 2.8583
Finetune Epoch 4 | Batch 450/629 | Avg Loss: 2.8688
Finetune Epoch 4 | Batch 500/629 | Avg Loss: 2.8640
Finetune Epoch 4 | Batch 550/629 | Avg Loss: 2.8678
Finetune Epoch 4 | Batch 600/629 | Avg Loss: 2.8654
--- End of Finetune Epoch 4 | Train Loss: 2.8611 | Val Loss: 3.5461 | Val Acc: 0.0249 | Val F1: 0.1806 | LR: 0.000025 ---
Finetune Epoch 5 | Batch 50/629 | Avg Loss: 2.5858
Finetune Epoch 5 | Batch 100/629 | Avg Loss: 2.6573
Finetune Epoch 5 | Batch 150/629 | Avg Loss: 2.6401
Finetune Epoch 5 | Batch 200/629 | Avg Loss: 2.6057
Finetune Epoch 5 | Batch 250/629 | Avg Loss: 2.6211
Finetune Epoch 5 | Batch 300/629 | Avg Loss: 2.6201
Finetune Epoch 5 | Batch 350/629 | Avg Loss: 2.6251
Finetune Epoch 5 | Batch 400/629 | Avg Loss: 2.6359
Finetune Epoch 5 | Batch 450/629 | Avg Loss: 2.6389
Finetune Epoch 5 | Batch 500/629 | Avg Loss: 2.6405
Finetune Epoch 5 | Batch 550/629 | Avg Loss: 2.6460
Finetune Epoch 5 | Batch 600/629 | Avg Loss: 2.6521
--- End of Finetune Epoch 5 | Train Loss: 2.6546 | Val Loss: 3.7846 | Val Acc: 0.0249 | Val F1: 0.1862 | LR: 0.000025 ---
Finetune Epoch 6 | Batch 50/629 | Avg Loss: 2.5506
Finetune Epoch 6 | Batch 100/629 | Avg Loss: 2.5494
Finetune Epoch 6 | Batch 150/629 | Avg Loss: 2.5696
Finetune Epoch 6 | Batch 200/629 | Avg Loss: 2.5575
Finetune Epoch 6 | Batch 250/629 | Avg Loss: 2.5548
Finetune Epoch 6 | Batch 300/629 | Avg Loss: 2.5676
Finetune Epoch 6 | Batch 350/629 | Avg Loss: 2.5671
Finetune Epoch 6 | Batch 400/629 | Avg Loss: 2.5556
Finetune Epoch 6 | Batch 450/629 | Avg Loss: 2.5551
Finetune Epoch 6 | Batch 500/629 | Avg Loss: 2.5621
Finetune Epoch 6 | Batch 550/629 | Avg Loss: 2.5598
Finetune Epoch 6 | Batch 600/629 | Avg Loss: 2.5598
--- End of Finetune Epoch 6 | Train Loss: 2.5641 | Val Loss: 3.9558 | Val Acc: 0.0255 | Val F1: 0.1902 | LR: 0.000025 ---

Fine-tuning finished.
Best fine-tuned QA model state_dict saved to models/TinyQA_20250915-024709/toy_llm_qasrl_finetuned.pth
Loading best model from models/TinyQA_20250915-024709/toy_llm_qasrl_finetuned.pth for final stats.
Final Validation Accuracy: 0.0101 | Final F1: 0.1781
Final training statistics saved to 'models/TinyQA_20250915-024709/finetune_stats.json'
Script finished.
